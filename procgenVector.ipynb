{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from procgen import ProcgenGym3Env\n",
    "from torchinfo import summary\n",
    "import core\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model = None\n",
    "player = None\n",
    "ppo = None\n",
    "env= None\n",
    "envKW = {}\n",
    "\n",
    "modelPath = \"models/\"\n",
    "def loadAll(fname, loadEnv=True):\n",
    "    model.load_state_dict(torch.load(modelPath + fname + \"/model.pth\"))\n",
    "    player.load_state_dict(torch.load(modelPath + fname + \"/player.pth\"))\n",
    "    ppo.load_state_dict(torch.load(modelPath + fname + \"/ppo.pth\"))\n",
    "    if loadEnv:\n",
    "        envKW = torch.load(modelPath + fname + \"/envKW.pth\")\n",
    "        env = ProcgenGym3Env(**envKW)\n",
    "        env.callmethod(\"set_state\", torch.load(modelPath + fname + \"/env_states.pth\"))\n",
    "    else:\n",
    "        player.reset()\n",
    "\n",
    "def saveAll(fname):\n",
    "    import os\n",
    "    os.makedirs(modelPath + fname, exist_ok=True)\n",
    "    torch.save(model.state_dict(), modelPath + fname + \"/model.pth\")\n",
    "    torch.save(player.state_dict(), modelPath + fname + \"/player.pth\")\n",
    "    torch.save(ppo.state_dict(), modelPath + fname + \"/ppo.pth\")\n",
    "    torch.save(envKW, modelPath + fname + \"/envKW.pth\")\n",
    "    torch.save(env.callmethod(\"get_state\"), modelPath + fname + \"/env_states.pth\")\n",
    "    torch.save(ppo.all_stats, modelPath + fname + \"/stats.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict(rgb=D256[64,64,3])\n",
      "D15[]\n"
     ]
    }
   ],
   "source": [
    "num_models = 2\n",
    "num_agents = 16\n",
    "envKW = core.getKW(num=num_models*num_agents, env_name=\"coinrun\", distribution_mode=\"easy\", paint_vel_info=True, use_backgrounds=False, restrict_themes=True)\n",
    "env = ProcgenGym3Env(**envKW)\n",
    "print(env.ob_space)\n",
    "print(env.ac_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "VectorModelValue                                   [2, 2, 15]                --\n",
       "├─ModuleList: 1-1                                  --                        --\n",
       "│    └─ViTValue: 2-1                               [2, 15]                   --\n",
       "│    │    └─VisionTransformer: 3-1                 --                        61,199\n",
       "│    │    └─ValueHead: 3-2                         [2, 1]                    33\n",
       "│    └─ViTValue: 2-2                               [2, 15]                   --\n",
       "│    │    └─VisionTransformer: 3-3                 --                        61,199\n",
       "│    │    └─ValueHead: 3-4                         [2, 1]                    33\n",
       "====================================================================================================\n",
       "Total params: 122,464\n",
       "Trainable params: 122,464\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 1.81\n",
       "====================================================================================================\n",
       "Input size (MB): 0.20\n",
       "Forward/backward pass size (MB): 12.11\n",
       "Params size (MB): 0.42\n",
       "Estimated Total Size (MB): 12.73\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from CVModels import CNNAgent, ViTValue, VectorModelValue\n",
    "model = ViTValue(depth=4, num_heads=4, embed_dim=32, mlp_ratio=4, valueHeadLayers=1).to(device)\n",
    "# model = ViTValue(depth=3, num_heads=4, embed_dim=16, mlp_ratio=4, valueHeadLayers=1)\n",
    "model = VectorModelValue(model, n=num_models).to(device)\n",
    "# model = CNNAgent([64, 64, 3], 15, channels=16, layers=[1,1,1,1], scale=[1,1,1,1], vheadLayers=1).to(device)\n",
    "model.train()\n",
    "summary(model, input_size=(2, 2, 3, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(modelPath + \"vitNegT8BigFin\" + \"/model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terminateReward -0.25 livingReward 0 discountedSumLiving 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Miniconda3\\envs\\torch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from PPO import VectorPPO\n",
    "from ProcgenPlayer import VectorPlayer\n",
    "\n",
    "rewardScale = 8.0\n",
    "terminateReward = 1 - 10.0 / rewardScale\n",
    "livingReward = 0\n",
    "print(\"terminateReward\", terminateReward, \"livingReward\", livingReward, \"discountedSumLiving\", livingReward / (1 - 0.99)) # if terminate reward > discountedSumLiving the agent will perfer to run into obstacles.\n",
    "player = VectorPlayer(env, num_agents=num_agents, num_models=num_models, epsilon=0.01, epsilon_decay=0.99, rewardScale=rewardScale, livingReward=0, terminateReward=terminateReward)\n",
    "ppo = VectorPPO(model, env, num_agents=num_agents, num_models=num_models, player=player, gamma=0.99, weight_decay=0.0, warmup_steps=10, train_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ppo.runGame()\n",
    "# loss = ppo.train(debug=True)\n",
    "# print(loss)\n",
    "# import torchviz\n",
    "# torchviz.make_dot(torch.sum(loss), params=dict(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loadAll(\"vitNegT8BigFin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episodeLength [245.3859649122807, 281.6842105263158] episodeReward [-0.03508771929824561, 0.0043859649122807015] \n",
      "epoch 0 steps 4096 \n",
      "loss tensor([0.0078, 0.0067], dtype=torch.float64) policy tensor([ 9.0856e-17, -6.3068e-16], dtype=torch.float64) \n",
      "value tensor([0.0078, 0.0067], dtype=torch.float64) entropy tensor([2.7060, 2.7061]) \n",
      "stale 0              \n",
      "episodeLength [172.94117647058823, 294.6470588235294] episodeReward [0.08823529411764706, 0.10294117647058823] \n",
      "epoch 10 steps 45056 \n",
      "loss tensor([0.0006, 0.0026], dtype=torch.float64) policy tensor([-0.0006,  0.0018], dtype=torch.float64) \n",
      "value tensor([0.0012, 0.0008], dtype=torch.float64) entropy tensor([2.6895, 2.6932]) \n",
      "stale 959              \n",
      "episodeLength [123.23529411764706, 225.52941176470588] episodeReward [0.04411764705882353, 0.18382352941176472] \n",
      "epoch 20 steps 86016 \n",
      "loss tensor([0.0035, 0.0012], dtype=torch.float64) policy tensor([0.0028, 0.0008], dtype=torch.float64) \n",
      "value tensor([0.0006, 0.0004], dtype=torch.float64) entropy tensor([2.6583, 2.6752]) \n",
      "stale 810              \n",
      "episodeLength [121.5, 245.5] episodeReward [0.14423076923076922, 0.23076923076923078] \n",
      "epoch 30 steps 126976 \n",
      "loss tensor([-0.0019,  0.0020], dtype=torch.float64) policy tensor([-0.0025,  0.0019], dtype=torch.float64) \n",
      "value tensor([0.0006, 0.0001], dtype=torch.float64) entropy tensor([2.6830, 2.6755]) \n",
      "stale 890              \n",
      "episodeLength [184.65, 177.55] episodeReward [0.1125, 0.1375] \n",
      "epoch 40 steps 167936 \n",
      "loss tensor([0.0012, 0.0061], dtype=torch.float64) policy tensor([0.0002, 0.0051], dtype=torch.float64) \n",
      "value tensor([0.0010, 0.0010], dtype=torch.float64) entropy tensor([2.6586, 2.6809]) \n",
      "stale 969              \n",
      "episodeLength [181.8181818181818, 541.6363636363636] episodeReward [-0.045454545454545456, 0.36363636363636365] \n",
      "epoch 50 steps 208896 \n",
      "loss tensor([0.0073, 0.0002], dtype=torch.float64) policy tensor([ 0.0070, -0.0004], dtype=torch.float64) \n",
      "value tensor([0.0003, 0.0006], dtype=torch.float64) entropy tensor([2.6739, 2.6615]) \n",
      "stale 920              \n",
      "episodeLength [304.93333333333334, 444.73333333333335] episodeReward [0.15, 0.18333333333333332] \n",
      "epoch 60 steps 249856 \n",
      "loss tensor([0.0029, 0.0006], dtype=torch.float64) policy tensor([2.6863e-03, 8.0187e-05], dtype=torch.float64) \n",
      "value tensor([0.0002, 0.0005], dtype=torch.float64) entropy tensor([2.6625, 2.6510]) \n",
      "stale 967              \n",
      "episodeLength [426.625, 324.625] episodeReward [-0.125, -0.125] \n",
      "epoch 70 steps 290816 \n",
      "loss tensor([0.0053, 0.0016], dtype=torch.float64) policy tensor([0.0052, 0.0016], dtype=torch.float64) \n",
      "value tensor([7.4051e-05, 7.8417e-05], dtype=torch.float64) entropy tensor([2.6693, 2.6657]) \n",
      "stale 903              \n",
      "episodeLength [500.0, 316.25] episodeReward [-0.125, -0.125] \n",
      "epoch 80 steps 331776 \n",
      "loss tensor([0.0029, 0.0002], dtype=torch.float64) policy tensor([2.6815e-03, 4.2951e-05], dtype=torch.float64) \n",
      "value tensor([0.0002, 0.0001], dtype=torch.float64) entropy tensor([2.6666, 2.6463]) \n",
      "stale 975              \n",
      "episodeLength [600.0, 400.0] episodeReward [-0.15, -0.1] \n",
      "epoch 90 steps 372736 \n",
      "loss tensor([-0.0013,  0.0021], dtype=torch.float64) policy tensor([-0.0014,  0.0020], dtype=torch.float64) \n",
      "value tensor([8.5261e-05, 5.1212e-05], dtype=torch.float64) entropy tensor([2.6483, 2.6453]) \n",
      "stale 951              \n",
      "episodeLength [400.0, 565.8] episodeReward [-0.1, -0.06666666666666667] \n",
      "epoch 100 steps 413696 \n",
      "loss tensor([-0.0037,  0.0005], dtype=torch.float64) policy tensor([-0.0038,  0.0002], dtype=torch.float64) \n",
      "value tensor([0.0001, 0.0003], dtype=torch.float64) entropy tensor([2.6134, 2.6583]) \n",
      "stale 999              \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Michael Einhorn\\Documents\\GTML\\RL\\procgenVector.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Michael%20Einhorn/Documents/GTML/RL/procgenVector.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m200\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Michael%20Einhorn/Documents/GTML/RL/procgenVector.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     ppo\u001b[39m.\u001b[39mrunGame()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Michael%20Einhorn/Documents/GTML/RL/procgenVector.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     ppo\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Michael%20Einhorn/Documents/GTML/RL/procgenVector.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Michael%20Einhorn/Documents/GTML/RL/procgenVector.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         \u001b[39m# print(\"episodeLength\", ppo.all_stats[-1][\"game/episodeLength\"], \"episodeReward\", ppo.all_stats[-1][\"game/episodeReward\"],\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Michael%20Einhorn/Documents/GTML/RL/procgenVector.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         \u001b[39m#       \"epoch\", ppo.all_stats[-1][\"epoch\"], \"steps\", ppo.all_stats[-1][\"steps\"], \u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Michael%20Einhorn/Documents/GTML/RL/procgenVector.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         \u001b[39m#       \"\\nloss\", ppo.all_stats[-1][\"ppo/loss/total\"].item(), \"policy\", ppo.all_stats[-1][\"ppo/loss/policy\"].item(), \u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Michael%20Einhorn/Documents/GTML/RL/procgenVector.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         \u001b[39m#       \"value\", ppo.all_stats[-1][\"ppo/loss/value\"].item(),\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Michael%20Einhorn/Documents/GTML/RL/procgenVector.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         \u001b[39m#       \"entropy\", ppo.all_stats[-1][\"ppo/policy/entropy\"].item())\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Michael%20Einhorn/Documents/GTML/RL/procgenVector.ipynb#X11sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mepisodeLength \u001b[39m\u001b[39m{\u001b[39;00mppo\u001b[39m.\u001b[39mall_stats[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mgame/episodeLength\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m episodeReward \u001b[39m\u001b[39m{\u001b[39;00mppo\u001b[39m.\u001b[39mall_stats[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mgame/episodeReward\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Michael%20Einhorn/Documents/GTML/RL/procgenVector.ipynb#X11sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mepoch \u001b[39m\u001b[39m{\u001b[39;00mppo\u001b[39m.\u001b[39mall_stats[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m steps \u001b[39m\u001b[39m{\u001b[39;00mppo\u001b[39m.\u001b[39mall_stats[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39msteps\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Michael%20Einhorn/Documents/GTML/RL/procgenVector.ipynb#X11sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mloss \u001b[39m\u001b[39m{\u001b[39;00mppo\u001b[39m.\u001b[39mall_stats[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mppo/loss/total\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m policy \u001b[39m\u001b[39m{\u001b[39;00mppo\u001b[39m.\u001b[39mall_stats[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mppo/loss/policy\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Michael%20Einhorn/Documents/GTML/RL/procgenVector.ipynb#X11sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mvalue \u001b[39m\u001b[39m{\u001b[39;00mppo\u001b[39m.\u001b[39mall_stats[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mppo/loss/value\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m entropy \u001b[39m\u001b[39m{\u001b[39;00mppo\u001b[39m.\u001b[39mall_stats[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mppo/policy/entropy\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Michael%20Einhorn/Documents/GTML/RL/procgenVector.ipynb#X11sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mstale \u001b[39m\u001b[39m{\u001b[39;00mppo\u001b[39m.\u001b[39mall_stats[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mgame/staleSteps\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m              \u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Michael Einhorn\\Documents\\GTML\\RL\\PPO.py:393\u001b[0m, in \u001b[0;36mVectorPPO.train\u001b[1;34m(self, debug)\u001b[0m\n\u001b[0;32m    378\u001b[0m value_mean, value_var \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(old_values, dim\u001b[39m=\u001b[39m\u001b[39mtuple\u001b[39m(i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(old_values\u001b[39m.\u001b[39mshape)))), torch\u001b[39m.\u001b[39mvar(old_values, dim\u001b[39m=\u001b[39m\u001b[39mtuple\u001b[39m(i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(old_values\u001b[39m.\u001b[39mshape))))\n\u001b[0;32m    380\u001b[0m stats \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[0;32m    381\u001b[0m     reward \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(reward, dim\u001b[39m=\u001b[39m\u001b[39mtuple\u001b[39m(i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(reward\u001b[39m.\u001b[39mshape)))),\n\u001b[0;32m    382\u001b[0m     loss\u001b[39m=\u001b[39m\u001b[39mdict\u001b[39m(policy\u001b[39m=\u001b[39mpg_loss, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    391\u001b[0m             clipfrac\u001b[39m=\u001b[39mvf_clipfrac, mean\u001b[39m=\u001b[39mvalue_mean, var\u001b[39m=\u001b[39mvalue_var),\n\u001b[0;32m    392\u001b[0m )\n\u001b[1;32m--> 393\u001b[0m stats \u001b[39m=\u001b[39m core\u001b[39m.\u001b[39;49mdict_to_cpu(core\u001b[39m.\u001b[39;49mflatten_dict(stats))\n\u001b[0;32m    394\u001b[0m \u001b[39m# step_stats.append(stats)\u001b[39;00m\n\u001b[0;32m    395\u001b[0m core\u001b[39m.\u001b[39mupdate_dict_add(step_stats, stats)\n",
      "File \u001b[1;32mc:\\Users\\Michael Einhorn\\Documents\\GTML\\RL\\core.py:224\u001b[0m, in \u001b[0;36mdict_to_cpu\u001b[1;34m(stats_dict)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m stats_dict\u001b[39m.\u001b[39mitems():\n\u001b[0;32m    223\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(v, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m--> 224\u001b[0m         new_dict[k] \u001b[39m=\u001b[39m v\u001b[39m.\u001b[39;49mdetach()\u001b[39m.\u001b[39;49mcpu()\n\u001b[0;32m    225\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    226\u001b[0m         new_dict[k] \u001b[39m=\u001b[39m v\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(200):\n",
    "    ppo.runGame()\n",
    "    ppo.train()\n",
    "    if i % 10 == 0:\n",
    "        # print(\"episodeLength\", ppo.all_stats[-1][\"game/episodeLength\"], \"episodeReward\", ppo.all_stats[-1][\"game/episodeReward\"],\n",
    "        #       \"epoch\", ppo.all_stats[-1][\"epoch\"], \"steps\", ppo.all_stats[-1][\"steps\"], \n",
    "        #       \"\\nloss\", ppo.all_stats[-1][\"ppo/loss/total\"].item(), \"policy\", ppo.all_stats[-1][\"ppo/loss/policy\"].item(), \n",
    "        #       \"value\", ppo.all_stats[-1][\"ppo/loss/value\"].item(),\n",
    "        #       \"entropy\", ppo.all_stats[-1][\"ppo/policy/entropy\"].item())\n",
    "        print(f\"episodeLength {ppo.all_stats[-1]['game/episodeLength']} episodeReward {ppo.all_stats[-1]['game/episodeReward']} \" + \n",
    "              f\"\\nepoch {ppo.all_stats[-1]['epoch']} steps {ppo.all_stats[-1]['steps']} \" +\n",
    "              f\"\\nloss {ppo.all_stats[-1]['ppo/loss/total']} policy {ppo.all_stats[-1]['ppo/loss/policy']} \" +\n",
    "              f\"\\nvalue {ppo.all_stats[-1]['ppo/loss/value']} entropy {ppo.all_stats[-1]['ppo/policy/entropy']} \" +\n",
    "              f\"\\nstale {ppo.all_stats[-1]['game/staleSteps']}              \")\n",
    "    # if i % 100 == 0:\n",
    "    #     stats = ppo.all_stats[-1]\n",
    "    #     for k, v in stats.items():\n",
    "    #         # if \"time\" in k:\n",
    "    #         print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saveAll(\"vitNegT8BigFin400\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ac488eaa353570522d4c04bd2cd8e3c67c3437ec54aafb89a01cbc7941828458"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
