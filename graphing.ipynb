{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import os\n",
    "from copy import deepcopy\n",
    "# %matplotlib inline\n",
    "# %matplotlib widget\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilons [0, 1]\n",
      "convN 10\n",
      "dicount 0.6\n",
      "syncBackups 1\n",
      "stochasticPolicy True\n",
      "envSeed 42\n",
      "alpha 1\n",
      "fedPs [2, 4, 6, 8, 10]\n"
     ]
    }
   ],
   "source": [
    "prefix = \"mdpvK\"\n",
    "jsonFileName = prefix + \".json\"\n",
    "jsonDict = {}\n",
    "\n",
    "epsilon = 1 # random vs policy actions while training\n",
    "epsilons = [0,1]\n",
    "convN = 10 # evaluates performance after convN episodes. Evaluates if converged after 10 performance evals\n",
    "discount = 0.6 # short term vs long term reward\n",
    "syncBackups = 1 # model backups between each sync\n",
    "stochasticPolicy = True # sample from softmax instead of using argmax\n",
    "envSeed = 42\n",
    "alpha = 1 # step size\n",
    "fedP = 2 # num agents\n",
    "fedPs = [2, 4, 6, 8, 10]\n",
    "syncBackupValues = [10000, 1000, 100, 10, 1]\n",
    "\n",
    "# jupyter nbconvert --no-input --to pdf graphing.ipynb\n",
    "\n",
    "print(\"epsilons \" + str(epsilons))\n",
    "print(\"convN \" + str(convN))\n",
    "print(\"dicount \" + str(discount))\n",
    "print(\"syncBackups \" + str(syncBackups))\n",
    "print(\"stochasticPolicy \" + str(stochasticPolicy))\n",
    "print(\"envSeed \" + str(envSeed))\n",
    "print(\"alpha \" + str(alpha))\n",
    "print(\"fedPs \" + str(fedPs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(jsonFileName):\n",
    "    jsonFile = open(jsonFileName)\n",
    "    jsonDict = json.load(jsonFile)\n",
    "    jsonFile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 0.1, 0.6, 1.0, 256, 880, True, 3701, 2)\n",
      "['10', '0.1', '0.6', '1.0', '256', '880', 'True', '3701', '2']\n",
      "['sims', 'diffs', 'rewards', 'endScoreStoch', 'avgRewArr', 'comment', 'avgRew', 'episodes', 'endScoreBell', 'endScoreRand', 'aggs', 'backups', 'epsToBackup', 'endScoreStochBell', 'endScore']\n",
      "learned  3898.6047529358507\n",
      "bell  3913.432567588432\n",
      "random  1281.841492486209\n"
     ]
    }
   ],
   "source": [
    "key = list(jsonDict.keys())[0]\n",
    "print(key)\n",
    "keyList = key[1:-1].split(\", \")\n",
    "print(keyList)\n",
    "itemKeys = list(jsonDict[key].keys())\n",
    "print(itemKeys)\n",
    "\n",
    "print(\"learned \", jsonDict[list(jsonDict)[0]][\"endScore\"])\n",
    "print(\"bell \", jsonDict[list(jsonDict)[0]][\"endScoreBell\"])\n",
    "print(\"random \", jsonDict[list(jsonDict)[0]][\"endScoreRand\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jsonDictKey = str((convN, alpha, discount, epsilon, fedP, syncBackups, stochasticPolicy, envSeed, trial))\n",
    "csvList = []\n",
    "labels = [\"convN\", \"alpha\", \"discount\", \"epsilon\", \"fedP\", \"syncBackups\", \"stochasticPolicy\", \"envSeed\", \"trial\"]\n",
    "itemKeys = ['sims', 'endScoreStoch', 'comment', 'avgRew', 'episodes', 'endScoreBell', 'endScoreRand', 'aggs', 'backups', 'endScoreStochBell', 'endScore']\n",
    "arrayKeys = ['diffs', 'rewards', 'avgRewArr', 'epsToBackup']\n",
    "summaryKeys = ['finalDiff', \"finalDiffAvg\", \"threshDiff\", \"threshRew\", \"threshEp\", \"threshBack\"]\n",
    "\n",
    "csvList.append(labels + itemKeys + summaryKeys)\n",
    "thresh = 0.1\n",
    "\n",
    "for key, value in jsonDict.items():\n",
    "    line = key[1:-1].split(\", \")\n",
    "    for itemKey in itemKeys:\n",
    "        if itemKey not in value:\n",
    "            line.append(0)\n",
    "        else:\n",
    "            line.append(value[itemKey])\n",
    "\n",
    "    # finalDiff\n",
    "    line.append(value[\"diffs\"][-1])\n",
    "    # finalDiffAvg\n",
    "    line.append(np.mean(value[\"diffs\"][-convN:]))\n",
    "    # find idx where diffs[i] < 0.1\n",
    "    idx = np.nonzero(np.array(value[\"diffs\"]) ** 2 < thresh)\n",
    "    # print(idx, idx[0].shape[0])\n",
    "    if idx[0].shape[0] == 0:\n",
    "        idx = int(value[\"episodes\"]) - 1\n",
    "    else:\n",
    "        idx = idx[0][0] * convN\n",
    "\n",
    "    # print(idx)\n",
    "    # print(len(value[\"diffs\"]))\n",
    "    # print(len(value[\"epsToBackup\"]))\n",
    "\n",
    "    # threshDiff\n",
    "    line.append(value[\"diffs\"][int(idx / convN)])\n",
    "    # print(value[\"diffs\"][0])\n",
    "    # threshRew\n",
    "    line.append(value[\"avgRewArr\"][int(idx / convN)])\n",
    "    # threshEp\n",
    "    line.append(idx)\n",
    "    \n",
    "    # threshBack\n",
    "    if idx+convN >= len(value[\"epsToBackup\"]):\n",
    "        line.append(value['backups'])\n",
    "    else:\n",
    "        line.append(value[\"epsToBackup\"][idx + convN])\n",
    "        \n",
    "    csvList.append(line)\n",
    "\n",
    "np.savetxt(prefix + \".csv\", csvList, delimiter=\",\", fmt=\"%s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jsonDictKey = str((convN, alpha, discount, epsilon, fedP, syncBackups, stochasticPolicy, envSeed, trial))\n",
    "csvList = []\n",
    "labels = [\"convN\", \"alpha\", \"discount\", \"epsilon\", \"fedP\", \"syncBackups\", \"stochasticPolicy\", \"envSeed\", \"trial\"]\n",
    "itemKeys = ['sims', 'endScoreStoch', 'comment', 'avgRew', 'episodes', 'endScoreBell', 'endScoreRand', 'aggs', 'backups', 'endScoreStochBell', 'endScore']\n",
    "arrayKeys = ['diffs', 'avgRewArr', 'epsToBackup']\n",
    "\n",
    "csvList.append(labels + itemKeys + arrayKeys)\n",
    "\n",
    "for key, value in jsonDict.items():\n",
    "    line = key[1:-1].split(\", \")\n",
    "    for itemKey in itemKeys:\n",
    "        if itemKey not in value:\n",
    "            line.append(0)\n",
    "        else:\n",
    "            line.append(value[itemKey])\n",
    "            \n",
    "    # hack to add step zero distance. Since q table is initialized to be a zero array it is a constant for all runs\n",
    "    # lineTemp = deepcopy(line)\n",
    "    # lineTemp.append(3.7055958469869537) # distance(0, bellman)\n",
    "    # lineTemp.append(0) # 0 reward\n",
    "    # lineTemp.append(0) # 0 backups\n",
    "    # csvList.append(lineTemp)\n",
    "\n",
    "    for i in range(len(value[\"diffs\"])):\n",
    "        # finalDiff, threshDiff, threshRew, threshEp, threshBack\n",
    "        lineTemp = deepcopy(line)\n",
    "\n",
    "        # find idx where diffs[i] < 0.1\n",
    "\n",
    "        # print(idx)\n",
    "        # print(len(value[\"diffs\"]))\n",
    "        # print(len(value[\"epsToBackup\"]))\n",
    "\n",
    "        lineTemp.append(value[\"diffs\"][i])\n",
    "        # print(value[\"diffs\"][0])\n",
    "        lineTemp.append(value[\"avgRewArr\"][i])\n",
    "        # lineTemp.append(idx)\n",
    "        if (i+1) * convN >= len(value[\"epsToBackup\"]):\n",
    "            lineTemp.append(value['backups'])\n",
    "        else:\n",
    "            lineTemp.append(value[\"epsToBackup\"][(i+1) * convN])\n",
    "        csvList.append(lineTemp)\n",
    "        \n",
    "np.savetxt(prefix + \"Traj.csv\", csvList, delimiter=\",\", fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file\n",
    "data = {}\n",
    "for epsilon in epsilons:\n",
    "    for syncBackups in syncBackupValues:\n",
    "        alpha = 1\n",
    "\n",
    "        epsSin = []\n",
    "        epsFed = {}\n",
    "        scoresSin = []\n",
    "        scoresFed = {}\n",
    "        scoresBell = []\n",
    "        scoresStochSin = []\n",
    "        scoresStochFed = {}\n",
    "        scoresStochBell = []\n",
    "        scoresRand = []\n",
    "        diffsSin = []\n",
    "        diffsFed = {}\n",
    "        alphas = []\n",
    "    \n",
    "    \n",
    "    \n",
    "        for a in range(10):\n",
    "            for fedP in fedPs:\n",
    "                if not fedP in epsFed:\n",
    "                    epsFed[fedP] = []\n",
    "                    scoresFed[fedP] = []\n",
    "                    diffsFed[fedP] = []\n",
    "                    scoresStochFed[fedP] = []\n",
    "\n",
    "                jsonDictKeyFed = str((convN, alpha, discount, epsilon, fedP, syncBackups, stochasticPolicy, envSeed))\n",
    "                if jsonDictKeyFed in jsonDict:\n",
    "#                     print(jsonDictKeyFed)\n",
    "                    out_dict = jsonDict[jsonDictKeyFed]\n",
    "                    epsFed[fedP].append(out_dict[\"episodes\"])\n",
    "                    scoresFed[fedP].append(out_dict[\"endScore\"]/100)\n",
    "                    diffsFed[fedP].append(np.mean(out_dict[\"diffs\"][-10:]))\n",
    "                    scoresStochFed[fedP].append(out_dict[\"endScoreStoch\"]/100)\n",
    "\n",
    "            jsonDictKeySin = str((convN, alpha, discount, epsilon, 1, -1, stochasticPolicy, envSeed))\n",
    "            if jsonDictKeySin in jsonDict:\n",
    "#                 print(jsonDictKeySin)\n",
    "            # print(jsonDictKeySin)\n",
    "                out_dict = jsonDict[jsonDictKeySin]\n",
    "\n",
    "                epsSin.append(out_dict[\"episodes\"])\n",
    "                scoresSin.append(out_dict[\"endScore\"]/100)\n",
    "                scoresBell.append(out_dict[\"endScoreBell\"]/100)\n",
    "                scoresRand.append(out_dict[\"endScoreRand\"]/100)\n",
    "                scoresStochSin.append(out_dict[\"endScoreStoch\"] / 100)\n",
    "                scoresStochBell.append(out_dict[\"endScoreStochBell\"] / 100)\n",
    "                diffsSin.append(np.mean(out_dict[\"diffs\"][-10:]))\n",
    "\n",
    "                alphas.append(alpha)\n",
    "\n",
    "                alpha /= 2\n",
    "                \n",
    "        data[str((convN, discount, epsilon, syncBackups, stochasticPolicy, envSeed))] = (epsSin,\n",
    "                    epsFed,  \n",
    "                    scoresSin,  \n",
    "                    scoresFed,  \n",
    "                    scoresBell,  \n",
    "                    scoresStochSin,  \n",
    "                    scoresStochFed,  \n",
    "                    scoresStochBell,  \n",
    "                    scoresRand,  \n",
    "                    diffsSin,  \n",
    "                    diffsFed,  \n",
    "                    alphas)\n",
    "                \n",
    "# print(\"data\")\n",
    "# for key in data.keys():\n",
    "#     print(key)\n",
    "\n",
    "def loadData(convN=convN, discount=discount, epsilon=epsilon, syncBackups=syncBackups, stochasticPolicy=stochasticPolicy, envSeed=envSeed):\n",
    "    global epsSin\n",
    "    global epsFed\n",
    "    global scoresSin\n",
    "    global scoresFed\n",
    "    global scoresBell\n",
    "    global scoresStochSin\n",
    "    global scoresStochFed\n",
    "    global scoresStochBell\n",
    "    global scoresRand \n",
    "    global diffsSin\n",
    "    global diffsFed \n",
    "    global alphas\n",
    "    (epsSin,\n",
    "    epsFed,  \n",
    "    scoresSin,  \n",
    "    scoresFed,  \n",
    "    scoresBell,  \n",
    "    scoresStochSin,  \n",
    "    scoresStochFed,  \n",
    "    scoresStochBell,  \n",
    "    scoresRand,  \n",
    "    diffsSin,  \n",
    "    diffsFed,  \n",
    "    alphas) = data[str((convN, discount, epsilon, syncBackups, stochasticPolicy, envSeed))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph is a comparison of learned models to the bellman model. A higher number of agents and smaller step size results in the lowest error to bellman. This trend appears linear on a log log plot for a variety of different numbers of agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "for i in range(len(epsilons)):\n",
    "    epsilon = epsilons[i]\n",
    "    loadData(epsilon=epsilon, syncBackups=1)\n",
    "    for placehold2 in [0]:\n",
    "        plt.subplot(1, 2, i+1)\n",
    "        plt.tight_layout(pad=3)\n",
    "        plotName = \"Q learning Difference from Bellman with Eps \" + str(epsilon)\n",
    "        plt.plot(alphas, np.array(diffsSin) ** 2, marker=\"x\", label=\"Single Agent\")\n",
    "#         print(diffsFed)\n",
    "        for fedP in fedPs:\n",
    "#             print(fedP)\n",
    "            plt.plot(alphas, np.array(diffsFed[fedP]) ** 2, marker=\"x\", label=str(fedP) + \" Agent\")\n",
    "        plt.legend()\n",
    "        plt.xlabel(\"alpha - step size\")\n",
    "        if i == 0:\n",
    "            plt.ylabel(\"distance squared to bellman q values\")\n",
    "\n",
    "        plt.title(plotName.replace(\"_\", \" \"))\n",
    "        # plt.ylim(0, 4)\n",
    "        \n",
    "#         plt.yscale('log', base=2)\n",
    "#         plt.xscale('log', base=2)\n",
    "plt.show()       \n",
    "\n",
    "# plot lines divided by 1 agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3d version of above plot by using number of agents as an additional axis instead of different series. This appears to be a plane on a log log log plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1\n",
    "loadData(epsilon=epsilon, syncBackups=1)\n",
    "plt.clf()\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter3D((alphas), ([1]*len(alphas)), np.array(diffsSin) ** 2, label=\"Single Agent\")\n",
    "for fedP in fedPs:\n",
    "    ax.scatter3D((alphas), [fedP]*len(alphas), np.array(diffsFed[fedP]) ** 2, label=str(fedP) + \" Agent\")\n",
    "ax.set_xlabel(\"alpha - step size\")\n",
    "ax.set_ylabel(\"p - Number of Agents\")\n",
    "ax.set_zlabel(\"distance squared to bellman q values\")\n",
    "plotName = \"Q learning Difference from Bellman with Eps \" + str(epsilon)\n",
    "plt.title(plotName)\n",
    "plt.show()  \n",
    "# ax.legend()\n",
    "# ax.set_xscale('log', base=2)\n",
    "# ax.set_zscale('log', base=2)\n",
    "# plt.show()\n",
    "\n",
    "# invert to show inverse curve as linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Reward when evaluated with a deterministic policy. Everything appears to be the same as bellman. This may present an issue with using reward to learn and get closer to bellman values for MDPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "for i in range(len(epsilons)):\n",
    "    epsilon = epsilons[i]\n",
    "    loadData(epsilon=epsilon, syncBackups=1)\n",
    "    for placehold2 in [0]:\n",
    "        plt.subplot(1, 2, i+1)\n",
    "        plt.tight_layout(pad=3)\n",
    "        plotName = \"Q learning Score with Eps \" + str(epsilon)\n",
    "        plt.plot(alphas, scoresSin, marker=\"x\", label=\"Single Agent\")\n",
    "        for fedP in fedPs:\n",
    "            plt.plot(alphas, scoresFed[fedP], marker=\"x\", label=str(fedP) + \" Agent\")\n",
    "        plt.hlines(np.mean(scoresBell), 0, 1, label=\"bellman score\")\n",
    "        plt.hlines(np.mean(scoresRand), 0, 1, label=\"random policy score\")\n",
    "        plt.legend()\n",
    "        plt.xlabel(\"alpha - step size\")\n",
    "        if i == 0:\n",
    "            plt.ylabel(\"Test Reward\")\n",
    "\n",
    "        plt.title(plotName.replace(\"_\", \" \"))\n",
    "        # plt.ylim(0, 4)\n",
    "        # plt.yscale('log', base=2)\n",
    "        # plt.xscale('log', base=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1\n",
    "loadData(epsilon=epsilon, syncBackups=1)\n",
    "plt.clf()\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "# print(scoresSin)\n",
    "ax.scatter3D(alphas, [1]*len(alphas), scoresSin, label=\"Single Agent\")\n",
    "for fedP in fedPs:\n",
    "    ax.scatter3D(alphas, [fedP]*len(alphas), scoresFed[fedP], label=str(fedP) + \" Agent\")\n",
    "ax.set_xlabel(\"alpha - step size\")\n",
    "ax.set_ylabel(\"p - Number of Agents\")\n",
    "ax.set_zlabel(\"Test Reward\")\n",
    "ax.ticklabel_format(useOffset=False)\n",
    "xx, yy = np.meshgrid(np.array(range(10))/10, range(10))\n",
    "z = xx - xx + yy - yy\n",
    "ax.plot_surface(xx, yy, z + np.mean(scoresBell), alpha=0.2, label=\"bellman score\")\n",
    "ax.plot_surface(xx, yy, z + np.mean(scoresRand), alpha=0.2, label=\"random policy score\")\n",
    "plt.title(plotName.replace(\"_\", \" \"))\n",
    "# ax.legend()\n",
    "# ax.set_xscale('log', base=2)\n",
    "# ax.set_zscale('log', base=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Reward when evaluated with a stochastic policy. This is the omly graph where epsilon 0 is noticably different than epsilon 1. Here the Q learning can get a higher score than the bellman Q values. Shows a rough increase in reward for more agents and smaller step sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "for i in range(len(epsilons)):\n",
    "    epsilon = epsilons[i]\n",
    "    loadData(epsilon=epsilon, syncBackups=1)\n",
    "    for placehold2 in [0]:\n",
    "        plt.subplot(1, 2, i+1)\n",
    "        plt.tight_layout(pad=3)\n",
    "        plotName = \"Q learning Score Stochastic Policy with Eps \" + str(epsilon)\n",
    "        plt.plot(alphas, scoresStochSin, marker=\"x\", label=\"Single Agent\")\n",
    "        for fedP in fedPs:\n",
    "            plt.plot(alphas, scoresStochFed[fedP], marker=\"x\", label=str(fedP) + \" Agent\")\n",
    "        plt.hlines(np.mean(scoresStochBell), 0, 1, label=\"bellman score\")\n",
    "        plt.hlines(np.mean(scoresRand), 0, 1, label=\"random policy score\")\n",
    "        plt.legend()\n",
    "        plt.xlabel(\"alpha - step size\")\n",
    "        if i == 0:\n",
    "            plt.ylabel(\"Test Reward\")\n",
    "\n",
    "        plt.title(plotName.replace(\"_\", \" \"))\n",
    "        # plt.ylim(0, 4)\n",
    "        # plt.yscale('log', base=2, base=2)\n",
    "        # plt.xscale('log', base=2, base=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1\n",
    "loadData(epsilon=epsilon, syncBackups=1)\n",
    "plt.clf()\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "# print(scoresSin)\n",
    "ax.scatter3D(alphas, [1]*len(alphas), scoresStochSin, label=\"Single Agent\")\n",
    "for fedP in fedPs:\n",
    "    ax.scatter3D(alphas, [fedP]*len(alphas), scoresStochFed[fedP], label=str(fedP) + \" Agent\")\n",
    "ax.set_xlabel(\"alpha - step size\")\n",
    "ax.set_ylabel(\"p - Number of Agents\")\n",
    "ax.set_zlabel(\"Test Reward\")\n",
    "ax.ticklabel_format(useOffset=False)\n",
    "xx, yy = np.meshgrid(np.array(range(10))/10, range(10))\n",
    "z = xx - xx + yy - yy\n",
    "ax.plot_surface(xx, yy, z + np.mean(scoresStochBell), alpha=0.2, label=\"bellman score\")\n",
    "ax.plot_surface(xx, yy, z + np.mean(scoresRand), alpha=0.2, label=\"random policy score\")\n",
    "plt.title(plotName.replace(\"_\", \" \"))\n",
    "# ax.legend()\n",
    "# ax.set_xscale('log', base=2)\n",
    "# ax.set_zscale('log', base=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convergence time, measured in model backups for a single agent (not all agents). This appears to show no speedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "for i in range(len(epsilons)):\n",
    "    epsilon = epsilons[i]\n",
    "    loadData(epsilon=epsilon, syncBackups=1)\n",
    "    for placehold2 in [0]:\n",
    "        plt.subplot(1, 2, i+1)\n",
    "        plt.tight_layout(pad=4)\n",
    "        plotName = \"Q learning Convergence Time with Eps \" + str(epsilon)\n",
    "        plt.plot(alphas, np.array(epsSin) * 100, marker=\"x\", label=\"Single Agent\")\n",
    "        for fedP in fedPs:\n",
    "            plt.plot(alphas, np.array(epsFed[fedP]) * 100, marker=\"x\", label=str(fedP) + \" Agent\")\n",
    "        plt.legend()\n",
    "        plt.xlabel(\"alpha - step size\")\n",
    "        if i == 0:\n",
    "            plt.ylabel(\"Number of backups for a single agent\")\n",
    "\n",
    "        plt.title(plotName.replace(\"_\", \" \"))\n",
    "        # plt.ylim(0, 4)\n",
    "        plt.yscale('log', base=2)\n",
    "        plt.xscale('log', base=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1\n",
    "loadData(epsilon=epsilon, syncBackups=1)\n",
    "plt.clf()\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter3D(alphas, [1]*len(alphas), epsSin, label=\"Single Agent\")\n",
    "for fedP in fedPs:\n",
    "    ax.scatter3D(alphas, [fedP]*len(alphas), epsFed[fedP], label=str(fedP) + \" Agent\")\n",
    "ax.set_xlabel(\"alpha - step size\")\n",
    "ax.set_ylabel(\"p - Number of Agents\")\n",
    "ax.set_zlabel(\"Number of backups for a single agent\")\n",
    "ax.ticklabel_format(useOffset=False)\n",
    "plt.title(plotName.replace(\"_\", \" \"))\n",
    "# ax.legend()\n",
    "# ax.set_xscale('log', base=2)\n",
    "# ax.set_zscale('log', base=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"jupyter nbconvert --no-input --to pdf graphing.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "\n",
    "for i in range(len(fedPs)):\n",
    "    for syncBackups in syncBackupValues:\n",
    "        fedP = fedPs[i]\n",
    "        epsilon = 1\n",
    "#         syncBackups = 1\n",
    "        loadData(epsilon=epsilon, syncBackups=syncBackups)\n",
    "        for placehold2 in [0]:\n",
    "            plt.subplot(2, len(fedPs)//2 + 1, i+1)\n",
    "            plt.tight_layout(pad=3)\n",
    "            plotName = \"Distance to Bellman Q function with \" + str(fedP) + \" Agents\"\n",
    "\n",
    "            plt.plot(alphas, np.array(diffsFed[fedP]) ** 2, marker=\"x\", label=\"K \" + str(syncBackups))\n",
    "\n",
    "            plt.legend()\n",
    "            plt.xlabel(\"alpha - step size\")\n",
    "            if i == 0:\n",
    "                plt.ylabel(\"final distance to bellman q values\")\n",
    "\n",
    "            plt.title(plotName.replace(\"_\", \" \"))\n",
    "            # plt.ylim(0, 4)\n",
    "#             plt.yscale('log', base=2)\n",
    "#             plt.xscale('log', base=2)\n",
    "            \n",
    "plt.rcParams['figure.figsize'] = [20, 20]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "for i in range(len(fedPs)):\n",
    "    for syncBackups in syncBackupValues:\n",
    "        fedP = fedPs[i]\n",
    "        epsilon = 1\n",
    "#         syncBackups = 1\n",
    "        loadData(epsilon=epsilon, syncBackups=syncBackups)\n",
    "        for placehold2 in [0]:\n",
    "            plt.subplot(2, len(fedPs)//2 + 1, i+1)\n",
    "            plt.tight_layout(pad=3)\n",
    "            plotName = \"Q learning Convergence Time with \" + str(fedP) + \" Agents\"\n",
    "\n",
    "            plt.plot(alphas, np.array(epsFed[fedP]) * 100, marker=\"x\", label=\"K \" + str(syncBackups))\n",
    "\n",
    "            plt.legend()\n",
    "            plt.xlabel(\"alpha - step size\")\n",
    "            if i == 0:\n",
    "                plt.ylabel(\"Number of backups for a single agent\")\n",
    "\n",
    "            plt.title(plotName.replace(\"_\", \" \"))\n",
    "            # plt.ylim(0, 4)\n",
    "            \n",
    "            plt.yscale('log', base=2)\n",
    "            plt.xscale('log', base=2)\n",
    "            \n",
    "plt.rcParams['figure.figsize'] = [15, 15]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epsilon = 1\n",
    "# loadData(epsilon=epsilon, syncBackups=1)\n",
    "# plt.clf()\n",
    "# fig = plt.figure()\n",
    "# ax = plt.axes(projection='3d')\n",
    "# ax.scatter3D(np.log2(alphas), np.log2([1]*len(alphas)), np.log2(diffsSin), label=\"Single Agent\")\n",
    "# for fedP in fedPs:\n",
    "#     ax.scatter3D(np.log2(alphas), np.log2([fedP]*len(alphas)), np.log2(diffsFed[fedP]), label=str(fedP) + \" Agent\")\n",
    "# ax.set_xlabel(\"alpha - step size log2\")\n",
    "# ax.set_ylabel(\"p - Number of Agents log2\")\n",
    "# ax.set_zlabel(\"final distance to bellman q values log2\")\n",
    "# plotName = \"Q learning Difference from Bellman with Eps \" + str(epsilon)\n",
    "# plt.title(plotName)\n",
    "# plt.show()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "ac488eaa353570522d4c04bd2cd8e3c67c3437ec54aafb89a01cbc7941828458"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
