{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from procgen import ProcgenGym3Env\n",
    "from torchinfo import summary\n",
    "import core\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model = None\n",
    "player = None\n",
    "ppo = None\n",
    "env= None\n",
    "envKW = {}\n",
    "\n",
    "modelPath = \"models/\"\n",
    "def loadAll(fname, loadEnv=True):\n",
    "    model.load_state_dict(torch.load(modelPath + fname + \"/model.pth\"))\n",
    "    player.load_state_dict(torch.load(modelPath + fname + \"/player.pth\"))\n",
    "    ppo.load_state_dict(torch.load(modelPath + fname + \"/ppo.pth\"))\n",
    "    if loadEnv:\n",
    "        envKW = torch.load(modelPath + fname + \"/envKW.pth\")\n",
    "        env = ProcgenGym3Env(**envKW)\n",
    "        env.callmethod(\"set_state\", torch.load(modelPath + fname + \"/env_states.pth\"))\n",
    "    else:\n",
    "        player.reset()\n",
    "\n",
    "def saveAll(fname):\n",
    "    import os\n",
    "    os.makedirs(modelPath + fname, exist_ok=True)\n",
    "    torch.save(model.state_dict(), modelPath + fname + \"/model.pth\")\n",
    "    torch.save(player.state_dict(), modelPath + fname + \"/player.pth\")\n",
    "    torch.save(ppo.state_dict(), modelPath + fname + \"/ppo.pth\")\n",
    "    torch.save(envKW, modelPath + fname + \"/envKW.pth\")\n",
    "    torch.save(env.callmethod(\"get_state\"), modelPath + fname + \"/env_states.pth\")\n",
    "    torch.save(ppo.all_stats, modelPath + fname + \"/stats.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict(rgb=D256[64,64,3])\n",
      "D15[]\n"
     ]
    }
   ],
   "source": [
    "num_agents = 16\n",
    "envKW = core.getKW(num=num_agents, env_name=\"coinrun\", distribution_mode=\"easy\", paint_vel_info=True, use_backgrounds=False, restrict_themes=True)\n",
    "env = ProcgenGym3Env(**envKW)\n",
    "print(env.ob_space)\n",
    "print(env.ac_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ViTValue                                 [2, 15]                   --\n",
       "├─VisionTransformer: 1-1                 --                        8,256\n",
       "│    └─PatchEmbed: 2-1                   [2, 256, 32]              --\n",
       "│    │    └─Conv2d: 3-1                  [2, 32, 16, 16]           1,568\n",
       "│    │    └─Identity: 3-2                [2, 256, 32]              --\n",
       "│    └─Dropout: 2-2                      [2, 257, 32]              --\n",
       "│    └─Identity: 2-3                     [2, 257, 32]              --\n",
       "│    └─Sequential: 2-4                   [2, 257, 32]              --\n",
       "│    │    └─Block: 3-3                   [2, 257, 32]              12,704\n",
       "│    │    └─Block: 3-4                   [2, 257, 32]              12,704\n",
       "│    │    └─Block: 3-5                   [2, 257, 32]              12,704\n",
       "│    │    └─Block: 3-6                   [2, 257, 32]              12,704\n",
       "│    └─LayerNorm: 2-5                    [2, 257, 32]              64\n",
       "│    └─Identity: 2-6                     [2, 32]                   --\n",
       "│    └─Linear: 2-7                       [2, 15]                   495\n",
       "├─ValueHead: 1-2                         [2, 1]                    --\n",
       "│    └─Linear: 2-8                       [2, 1]                    33\n",
       "==========================================================================================\n",
       "Total params: 61,232\n",
       "Trainable params: 61,232\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.91\n",
       "==========================================================================================\n",
       "Input size (MB): 0.10\n",
       "Forward/backward pass size (MB): 6.05\n",
       "Params size (MB): 0.21\n",
       "Estimated Total Size (MB): 6.36\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from CVModels import CNNAgent, ViTValue\n",
    "model = ViTValue(depth=4, num_heads=4, embed_dim=32, mlp_ratio=4, valueHeadLayers=1).to(device)\n",
    "# model = ViTValue(depth=3, num_heads=4, embed_dim=16, mlp_ratio=4, valueHeadLayers=1).to(device)\n",
    "# model = CNNAgent([64, 64, 3], 15, channels=16, layers=[1,1,1,1], scale=[1,1,1,1], vheadLayers=1).to(device)\n",
    "model.train()\n",
    "summary(model, input_size=(2, 3, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(modelPath + \"vitNegT8BigFin\" + \"/model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terminateReward -0.25 livingReward 0 discountedSumLiving 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Miniconda3\\envs\\torch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from PPO import PPO\n",
    "import ProcgenPlayer\n",
    "\n",
    "rewardScale = 8.0\n",
    "terminateReward = 1 - 10.0 / rewardScale\n",
    "livingReward = 0\n",
    "print(\"terminateReward\", terminateReward, \"livingReward\", livingReward, \"discountedSumLiving\", livingReward / (1 - 0.99)) # if terminate reward > discountedSumLiving the agent will perfer to run into obstacles.\n",
    "player = ProcgenPlayer.Player(env, num_agents=num_agents, epsilon=0.01, epsilon_decay=0.99, rewardScale=rewardScale, livingReward=0, terminateReward=terminateReward)\n",
    "ppo = PPO(model, env, num_agents=num_agents, player=player, gamma=0.99, weight_decay=0.0, warmup_steps=10, train_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ppo.runGame()\n",
    "# loss = ppo.train(debug=True)\n",
    "# print(loss)\n",
    "# import torchviz\n",
    "# torchviz.make_dot(loss, params=dict(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loadAll(\"vitNegT8BigFin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episodeLength 842.9444 episodeReward -0.1806 epoch 0 steps 4096 loss 0.0088 policy -0.0000 value 0.0088 entropy 2.7063 stale 0              \n",
      "episodeLength 511.0000 episodeReward 0.3750 epoch 1 steps 8192 loss 0.0002 policy -0.0029 value 0.0032 entropy 2.7026 stale 997              \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Michael Einhorn\\Documents\\GTML\\RL\\procgen.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Michael%20Einhorn/Documents/GTML/RL/procgen.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m200\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Michael%20Einhorn/Documents/GTML/RL/procgen.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     ppo\u001b[39m.\u001b[39mrunGame()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Michael%20Einhorn/Documents/GTML/RL/procgen.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     ppo\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Michael%20Einhorn/Documents/GTML/RL/procgen.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Michael%20Einhorn/Documents/GTML/RL/procgen.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         \u001b[39m# print(\"episodeLength\", ppo.all_stats[-1][\"game/episodeLength\"], \"episodeReward\", ppo.all_stats[-1][\"game/episodeReward\"],\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Michael%20Einhorn/Documents/GTML/RL/procgen.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         \u001b[39m#       \"epoch\", ppo.all_stats[-1][\"epoch\"], \"steps\", ppo.all_stats[-1][\"steps\"], \u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Michael%20Einhorn/Documents/GTML/RL/procgen.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         \u001b[39m#       \"\\nloss\", ppo.all_stats[-1][\"ppo/loss/total\"].item(), \"policy\", ppo.all_stats[-1][\"ppo/loss/policy\"].item(), \u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Michael%20Einhorn/Documents/GTML/RL/procgen.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         \u001b[39m#       \"value\", ppo.all_stats[-1][\"ppo/loss/value\"].item(),\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Michael%20Einhorn/Documents/GTML/RL/procgen.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         \u001b[39m#       \"entropy\", ppo.all_stats[-1][\"ppo/policy/entropy\"].item())\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Michael%20Einhorn/Documents/GTML/RL/procgen.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mepisodeLength \u001b[39m\u001b[39m{\u001b[39;00mppo\u001b[39m.\u001b[39mall_stats[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mgame/episodeLength\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m episodeReward \u001b[39m\u001b[39m{\u001b[39;00mppo\u001b[39m.\u001b[39mall_stats[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mgame/episodeReward\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Michael%20Einhorn/Documents/GTML/RL/procgen.ipynb#X10sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mepoch \u001b[39m\u001b[39m{\u001b[39;00mppo\u001b[39m.\u001b[39mall_stats[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m steps \u001b[39m\u001b[39m{\u001b[39;00mppo\u001b[39m.\u001b[39mall_stats[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39msteps\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Michael%20Einhorn/Documents/GTML/RL/procgen.ipynb#X10sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mloss \u001b[39m\u001b[39m{\u001b[39;00mppo\u001b[39m.\u001b[39mall_stats[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mppo/loss/total\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mitem()\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m policy \u001b[39m\u001b[39m{\u001b[39;00mppo\u001b[39m.\u001b[39mall_stats[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mppo/loss/policy\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mitem()\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Michael%20Einhorn/Documents/GTML/RL/procgen.ipynb#X10sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mvalue \u001b[39m\u001b[39m{\u001b[39;00mppo\u001b[39m.\u001b[39mall_stats[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mppo/loss/value\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mitem()\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m entropy \u001b[39m\u001b[39m{\u001b[39;00mppo\u001b[39m.\u001b[39mall_stats[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mppo/policy/entropy\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mitem()\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Michael%20Einhorn/Documents/GTML/RL/procgen.ipynb#X10sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstale \u001b[39m\u001b[39m{\u001b[39;00mppo\u001b[39m.\u001b[39mall_stats[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mgame/staleSteps\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m              \u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Michael Einhorn\\Documents\\GTML\\RL\\PPO.py:155\u001b[0m, in \u001b[0;36mPPO.train\u001b[1;34m(self, debug)\u001b[0m\n\u001b[0;32m    153\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtiming[\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtime/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39malg_name\u001b[39m}\u001b[39;00m\u001b[39m/backward\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t\n\u001b[0;32m    154\u001b[0m     t \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m--> 155\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m    156\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtiming[\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtime/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39malg_name\u001b[39m}\u001b[39;00m\u001b[39m/optim\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t\n\u001b[0;32m    158\u001b[0m t \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\envs\\torch\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:65\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     64\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\envs\\torch\\lib\\site-packages\\torch\\optim\\optimizer.py:109\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    107\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m    108\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 109\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\envs\\torch\\lib\\site-packages\\transformers\\optimization.py:370\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    367\u001b[0m     bias_correction2 \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m \u001b[39m-\u001b[39m beta2 \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m state[\u001b[39m\"\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    368\u001b[0m     step_size \u001b[39m=\u001b[39m step_size \u001b[39m*\u001b[39m math\u001b[39m.\u001b[39msqrt(bias_correction2) \u001b[39m/\u001b[39m bias_correction1\n\u001b[1;32m--> 370\u001b[0m p\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49mstep_size)\n\u001b[0;32m    372\u001b[0m \u001b[39m# Just adding the square of the weights to the loss function is *not*\u001b[39;00m\n\u001b[0;32m    373\u001b[0m \u001b[39m# the correct way of using L2 regularization/weight decay with Adam,\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[39m# since that will interact with the m and v parameters in strange ways.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[39m# of the weights to the loss with plain (non-momentum) SGD.\u001b[39;00m\n\u001b[0;32m    379\u001b[0m \u001b[39m# Add weight decay at the end (fixed version)\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \u001b[39mif\u001b[39;00m group[\u001b[39m\"\u001b[39m\u001b[39mweight_decay\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m0.0\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(200):\n",
    "    ppo.runGame()\n",
    "    ppo.train()\n",
    "    if i % 10 == 0:\n",
    "        # print(\"episodeLength\", ppo.all_stats[-1][\"game/episodeLength\"], \"episodeReward\", ppo.all_stats[-1][\"game/episodeReward\"],\n",
    "        #       \"epoch\", ppo.all_stats[-1][\"epoch\"], \"steps\", ppo.all_stats[-1][\"steps\"], \n",
    "        #       \"\\nloss\", ppo.all_stats[-1][\"ppo/loss/total\"].item(), \"policy\", ppo.all_stats[-1][\"ppo/loss/policy\"].item(), \n",
    "        #       \"value\", ppo.all_stats[-1][\"ppo/loss/value\"].item(),\n",
    "        #       \"entropy\", ppo.all_stats[-1][\"ppo/policy/entropy\"].item())\n",
    "        print(f\"episodeLength {ppo.all_stats[-1]['game/episodeLength']:.4f} episodeReward {ppo.all_stats[-1]['game/episodeReward']:.4f} \" + \n",
    "              f\"epoch {ppo.all_stats[-1]['epoch']} steps {ppo.all_stats[-1]['steps']} \" +\n",
    "              f\"loss {ppo.all_stats[-1]['ppo/loss/total'].item():.4f} policy {ppo.all_stats[-1]['ppo/loss/policy'].item():.4f} \" +\n",
    "              f\"value {ppo.all_stats[-1]['ppo/loss/value'].item():.4f} entropy {ppo.all_stats[-1]['ppo/policy/entropy'].item():.4f} \" +\n",
    "              f\"stale {ppo.all_stats[-1]['game/staleSteps']}              \")\n",
    "    else:\n",
    "        print(f\"episodeLength {ppo.all_stats[-1]['game/episodeLength']:.4f} episodeReward {ppo.all_stats[-1]['game/episodeReward']:.4f} \" + \n",
    "              f\"epoch {ppo.all_stats[-1]['epoch']} steps {ppo.all_stats[-1]['steps']} \" +\n",
    "              f\"loss {ppo.all_stats[-1]['ppo/loss/total'].item():.4f} policy {ppo.all_stats[-1]['ppo/loss/policy'].item():.4f} \" +\n",
    "              f\"value {ppo.all_stats[-1]['ppo/loss/value'].item():.4f} entropy {ppo.all_stats[-1]['ppo/policy/entropy'].item():.4f} \" +\n",
    "              f\"stale {ppo.all_stats[-1]['game/staleSteps']}              \", end=\"\\r\")\n",
    "    # if i % 100 == 0:\n",
    "    #     stats = ppo.all_stats[-1]\n",
    "    #     for k, v in stats.items():\n",
    "    #         # if \"time\" in k:\n",
    "    #         print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saveAll(\"vitNegT8BigFin400\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment Notes\n",
    "\n",
    "1. coin run hard\n",
    "\n",
    "Higher reward per episode is better, and shorter episodes are better. Not sure if reward per timestep consolidates these.\n",
    "The model can learn, and actively persue the reward but get lower reward because it now dies more. Though this is clearly better than headslamming a wall for several hundred  timesteps before randomly jumping into a coin. Shorter episodes also means higher sample size on reward per episode.\n",
    "\n",
    "1.1 VIT 15k\n",
    "\n",
    "LR 1e-3 warmup and cosine decay\n",
    "\n",
    "Weight decay 0.01, living reward -0.001\n",
    "Doesn't like to jump doesn't seem to avoid enemies.\n",
    "\n",
    "Weight decay 0.01\n",
    "Doesn't like to jump\n",
    "\n",
    "No wd or livrew\n",
    "Gets stuck going right\n",
    "\n",
    "Removed Background and theme variation\n",
    "got up to 87% rewards, though most epochs are ~70%\n",
    "Very jumpy, spends almost no time on ground\n",
    "Does not seem to avoid obstacles, always jump appears to be a passive strategy\n",
    "\n",
    "Negative terminate reward. +1 for coin, -1 for enemy or timeout.\n",
    "never got better than -1 reward.\n",
    "\n",
    "Negative terminate reward. +1 for coin, -0.25 for enemy or timeout.\n",
    "Improves for 200 epochs, gets worse around 350. Gets to around 0.5 which is 75% of coins.\n",
    "\n",
    "1.2 VIT 60k\n",
    "2 Layer value head degrades performance, is about 2k extra params. Vf loss gets larger when training.\n",
    "\n",
    "Negative terminate reward. +1 for coin, -0.25 for enemy or timeout.\n",
    "Did well until an over 1k step episode, spiked vf error.\n",
    "\n",
    "Keep running environment on extra steps until every episode finishes.\n",
    "Stale steps started very high around 90, but by epoch 70 it was down to 140. 256 steps per epoch, so a value higher than 256 is an entirely stale epoch.\n",
    "Was considering capping the amount of stale steps, but it seems to work without this after a bit.\n",
    "Reward is steadily increasing.\n",
    "Still very jumpy but not as much as other models. Stays on ground for a few frames. Does not seem to actively avoid obstacles, though appears to be favoring landing on boxes over the ground which are always safe.\n",
    "400 epochs, doesn't favor boxes as much, might be a bit better at jumping over gaps and avoiding obstacles.\n",
    "\n",
    "Add living reward, train from previous model.\n",
    "Discounted Sum of infinite living rewards is less than half the terminate reward so the agent should not perfer to hit an obstacle and end the episode.\n",
    "Reduced high episode lenght faster by epoch 50\n",
    "1000 step episodes at epoch 140 and derailed\n",
    "\n",
    "For all above there was an issue with compute advantages where the first advantage of an episode would be set to zero. Probably had little impact.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ac488eaa353570522d4c04bd2cd8e3c67c3437ec54aafb89a01cbc7941828458"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
