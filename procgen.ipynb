{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from procgen import ProcgenGym3Env\n",
    "from torchinfo import summary\n",
    "import core\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model = None\n",
    "player = None\n",
    "ppo = None\n",
    "env= None\n",
    "envKW = {}\n",
    "\n",
    "modelPath = \"models/\"\n",
    "def loadAll(fname, loadEnv=True):\n",
    "    model.load_state_dict(torch.load(modelPath + fname + \"/model.pth\"))\n",
    "    player.load_state_dict(torch.load(modelPath + fname + \"/player.pth\"))\n",
    "    ppo.load_state_dict(torch.load(modelPath + fname + \"/ppo.pth\"))\n",
    "    if loadEnv:\n",
    "        envKW = torch.load(modelPath + fname + \"/envKW.pth\")\n",
    "        env = ProcgenGym3Env(**envKW)\n",
    "        env.callmethod(\"set_state\", torch.load(modelPath + fname + \"/env_states.pth\"))\n",
    "    else:\n",
    "        player.reset()\n",
    "\n",
    "def saveAll(fname):\n",
    "    import os\n",
    "    os.makedirs(modelPath + fname, exist_ok=True)\n",
    "    torch.save(model.state_dict(), modelPath + fname + \"/model.pth\")\n",
    "    torch.save(player.state_dict(), modelPath + fname + \"/player.pth\")\n",
    "    torch.save(ppo.state_dict(), modelPath + fname + \"/ppo.pth\")\n",
    "    torch.save(envKW, modelPath + fname + \"/envKW.pth\")\n",
    "    torch.save(env.callmethod(\"get_state\"), modelPath + fname + \"/env_states.pth\")\n",
    "    torch.save(ppo.all_stats, modelPath + fname + \"/stats.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_agents = 16\n",
    "envKW = core.getKW(num=num_agents, env_name=\"coinrun\", distribution_mode=\"easy\", paint_vel_info=True, use_backgrounds=False, restrict_themes=True)\n",
    "env = ProcgenGym3Env(**envKW)\n",
    "print(env.ob_space)\n",
    "print(env.ac_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CVModels import CNNAgent, ViTValue\n",
    "model = ViTValue(depth=4, num_heads=4, embed_dim=32, mlp_ratio=4, valueHeadLayers=1).to(device)\n",
    "# model = ViTValue(depth=3, num_heads=4, embed_dim=16, mlp_ratio=4, valueHeadLayers=1).to(device)\n",
    "# model = CNNAgent([64, 64, 3], 15, channels=16, layers=[1,1,1,1], scale=[1,1,1,1], vheadLayers=1).to(device)\n",
    "model.train()\n",
    "summary(model, input_size=(2, 3, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(modelPath + \"vitNegT8BigFin\" + \"/model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PPO import PPO\n",
    "import ProcgenPlayer\n",
    "\n",
    "rewardScale = 8.0\n",
    "terminateReward = 1 - 10.0 / rewardScale\n",
    "livingReward = 0\n",
    "print(\"terminateReward\", terminateReward, \"livingReward\", livingReward, \"discountedSumLiving\", livingReward / (1 - 0.99)) # if terminate reward > discountedSumLiving the agent will perfer to run into obstacles.\n",
    "player = ProcgenPlayer.Player(env, num_agents=num_agents, epsilon=0.01, epsilon_decay=0.99, rewardScale=rewardScale, livingReward=0, terminateReward=terminateReward)\n",
    "ppo = PPO(model, env, num_agents=num_agents, player=player, gamma=0.99, weight_decay=0.0, warmup_steps=10, train_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ppo.runGame()\n",
    "# loss = ppo.train(debug=True)\n",
    "# print(loss)\n",
    "# import torchviz\n",
    "# torchviz.make_dot(loss, params=dict(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loadAll(\"vitNegT8BigFin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(200):\n",
    "    ppo.runGame()\n",
    "    ppo.train()\n",
    "    if i % 10 == 0:\n",
    "        # print(\"episodeLength\", ppo.all_stats[-1][\"game/episodeLength\"], \"episodeReward\", ppo.all_stats[-1][\"game/episodeReward\"],\n",
    "        #       \"epoch\", ppo.all_stats[-1][\"epoch\"], \"steps\", ppo.all_stats[-1][\"steps\"], \n",
    "        #       \"\\nloss\", ppo.all_stats[-1][\"ppo/loss/total\"].item(), \"policy\", ppo.all_stats[-1][\"ppo/loss/policy\"].item(), \n",
    "        #       \"value\", ppo.all_stats[-1][\"ppo/loss/value\"].item(),\n",
    "        #       \"entropy\", ppo.all_stats[-1][\"ppo/policy/entropy\"].item())\n",
    "        print(f\"episodeLength {ppo.all_stats[-1]['game/episodeLength']:.4f} episodeReward {ppo.all_stats[-1]['game/episodeReward']:.4f} \" + \n",
    "              f\"epoch {ppo.all_stats[-1]['epoch']} steps {ppo.all_stats[-1]['steps']} \" +\n",
    "              f\"loss {ppo.all_stats[-1]['ppo/loss/total'].item():.4f} policy {ppo.all_stats[-1]['ppo/loss/policy'].item():.4f} \" +\n",
    "              f\"value {ppo.all_stats[-1]['ppo/loss/value'].item():.4f} entropy {ppo.all_stats[-1]['ppo/policy/entropy'].item():.4f} \" +\n",
    "              f\"stale {ppo.all_stats[-1]['game/staleSteps']}              \")\n",
    "    else:\n",
    "        print(f\"episodeLength {ppo.all_stats[-1]['game/episodeLength']:.4f} episodeReward {ppo.all_stats[-1]['game/episodeReward']:.4f} \" + \n",
    "              f\"epoch {ppo.all_stats[-1]['epoch']} steps {ppo.all_stats[-1]['steps']} \" +\n",
    "              f\"loss {ppo.all_stats[-1]['ppo/loss/total'].item():.4f} policy {ppo.all_stats[-1]['ppo/loss/policy'].item():.4f} \" +\n",
    "              f\"value {ppo.all_stats[-1]['ppo/loss/value'].item():.4f} entropy {ppo.all_stats[-1]['ppo/policy/entropy'].item():.4f} \" +\n",
    "              f\"stale {ppo.all_stats[-1]['game/staleSteps']}              \", end=\"\\r\")\n",
    "    # if i % 100 == 0:\n",
    "    #     stats = ppo.all_stats[-1]\n",
    "    #     for k, v in stats.items():\n",
    "    #         # if \"time\" in k:\n",
    "    #         print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saveAll(\"vitNegT8BigFin400\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment Notes\n",
    "\n",
    "1. coin run hard\n",
    "\n",
    "Higher reward per episode is better, and shorter episodes are better. Not sure if reward per timestep consolidates these.\n",
    "The model can learn, and actively persue the reward but get lower reward because it now dies more. Though this is clearly better than headslamming a wall for several hundred  timesteps before randomly jumping into a coin. Shorter episodes also means higher sample size on reward per episode.\n",
    "\n",
    "1.1 VIT 15k\n",
    "\n",
    "LR 1e-3 warmup and cosine decay\n",
    "\n",
    "Weight decay 0.01, living reward -0.001\n",
    "Doesn't like to jump doesn't seem to avoid enemies.\n",
    "\n",
    "Weight decay 0.01\n",
    "Doesn't like to jump\n",
    "\n",
    "No wd or livrew\n",
    "Gets stuck going right\n",
    "\n",
    "Removed Background and theme variation\n",
    "got up to 87% rewards, though most epochs are ~70%\n",
    "Very jumpy, spends almost no time on ground\n",
    "Does not seem to avoid obstacles, always jump appears to be a passive strategy\n",
    "\n",
    "Negative terminate reward. +1 for coin, -1 for enemy or timeout.\n",
    "never got better than -1 reward.\n",
    "\n",
    "Negative terminate reward. +1 for coin, -0.25 for enemy or timeout.\n",
    "Improves for 200 epochs, gets worse around 350. Gets to around 0.5 which is 75% of coins.\n",
    "\n",
    "1.2 VIT 60k\n",
    "2 Layer value head degrades performance, is about 2k extra params. Vf loss gets larger when training.\n",
    "\n",
    "Negative terminate reward. +1 for coin, -0.25 for enemy or timeout.\n",
    "Did well until an over 1k step episode, spiked vf error.\n",
    "\n",
    "Keep running environment on extra steps until every episode finishes.\n",
    "Stale steps started very high around 90, but by epoch 70 it was down to 140. 256 steps per epoch, so a value higher than 256 is an entirely stale epoch.\n",
    "Was considering capping the amount of stale steps, but it seems to work without this after a bit.\n",
    "Reward is steadily increasing.\n",
    "Still very jumpy but not as much as other models. Stays on ground for a few frames. Does not seem to actively avoid obstacles, though appears to be favoring landing on boxes over the ground which are always safe.\n",
    "400 epochs, doesn't favor boxes as much, might be a bit better at jumping over gaps and avoiding obstacles.\n",
    "\n",
    "Add living reward, train from previous model.\n",
    "Discounted Sum of infinite living rewards is less than half the terminate reward so the agent should not perfer to hit an obstacle and end the episode.\n",
    "Reduced high episode lenght faster by epoch 50\n",
    "1000 step episodes at epoch 140 and derailed\n",
    "\n",
    "For all above there was an issue with compute advantages where the first advantage of an episode would be set to zero. Probably had little impact.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ac488eaa353570522d4c04bd2cd8e3c67c3437ec54aafb89a01cbc7941828458"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
