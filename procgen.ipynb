{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from procgen import ProcgenGym3Env\n",
    "from torchinfo import summary\n",
    "import core\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model = None\n",
    "player = None\n",
    "ppo = None\n",
    "env= None\n",
    "envKW = {}\n",
    "\n",
    "modelPath = \"models/\"\n",
    "def loadAll(fname, loadEnv=True):\n",
    "    model.load_state_dict(torch.load(modelPath + fname + \"/model.pth\"))\n",
    "    player.load_state_dict(torch.load(modelPath + fname + \"/player.pth\"))\n",
    "    ppo.load_state_dict(torch.load(modelPath + fname + \"/ppo.pth\"))\n",
    "    if loadEnv:\n",
    "        envKW = torch.load(modelPath + fname + \"/envKW.pth\")\n",
    "        env = ProcgenGym3Env(**envKW)\n",
    "        env.callmethod(\"set_state\", torch.load(modelPath + fname + \"/env_states.pth\"))\n",
    "    else:\n",
    "        player.reset()\n",
    "\n",
    "def saveAll(fname):\n",
    "    import os\n",
    "    os.makedirs(modelPath + fname, exist_ok=True)\n",
    "    torch.save(model.state_dict(), modelPath + fname + \"/model.pth\")\n",
    "    torch.save(player.state_dict(), modelPath + fname + \"/player.pth\")\n",
    "    torch.save(ppo.state_dict(), modelPath + fname + \"/ppo.pth\")\n",
    "    torch.save(envKW, modelPath + fname + \"/envKW.pth\")\n",
    "    torch.save(env.callmethod(\"get_state\"), modelPath + fname + \"/env_states.pth\")\n",
    "    torch.save(ppo.all_stats, modelPath + fname + \"/stats.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict(rgb=D256[64,64,3])\n",
      "D15[]\n"
     ]
    }
   ],
   "source": [
    "num_agents = 16\n",
    "envKW = core.getKW(num=num_agents, env_name=\"coinrun\", distribution_mode=\"easy\", paint_vel_info=True, use_backgrounds=False, restrict_themes=True)\n",
    "env = ProcgenGym3Env(**envKW)\n",
    "print(env.ob_space)\n",
    "print(env.ac_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ViTValue                                 [2, 15]                   --\n",
       "├─VisionTransformer: 1-1                 --                        8,256\n",
       "│    └─PatchEmbed: 2-1                   [2, 256, 32]              --\n",
       "│    │    └─Conv2d: 3-1                  [2, 32, 16, 16]           1,568\n",
       "│    │    └─Identity: 3-2                [2, 256, 32]              --\n",
       "│    └─Dropout: 2-2                      [2, 257, 32]              --\n",
       "│    └─Identity: 2-3                     [2, 257, 32]              --\n",
       "│    └─Sequential: 2-4                   [2, 257, 32]              --\n",
       "│    │    └─Block: 3-3                   [2, 257, 32]              12,704\n",
       "│    │    └─Block: 3-4                   [2, 257, 32]              12,704\n",
       "│    │    └─Block: 3-5                   [2, 257, 32]              12,704\n",
       "│    │    └─Block: 3-6                   [2, 257, 32]              12,704\n",
       "│    └─LayerNorm: 2-5                    [2, 257, 32]              64\n",
       "│    └─Identity: 2-6                     [2, 32]                   --\n",
       "│    └─Linear: 2-7                       [2, 15]                   495\n",
       "├─ValueHead: 1-2                         [2, 1]                    --\n",
       "│    └─Linear: 2-8                       [2, 1]                    33\n",
       "==========================================================================================\n",
       "Total params: 61,232\n",
       "Trainable params: 61,232\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.91\n",
       "==========================================================================================\n",
       "Input size (MB): 0.10\n",
       "Forward/backward pass size (MB): 6.05\n",
       "Params size (MB): 0.21\n",
       "Estimated Total Size (MB): 6.36\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from CVModels import CNNAgent, ViTValue\n",
    "model = ViTValue(depth=4, num_heads=4, embed_dim=32, mlp_ratio=4, valueHeadLayers=1).to(device)\n",
    "# model = ViTValue(depth=3, num_heads=4, embed_dim=16, mlp_ratio=4, valueHeadLayers=1).to(device)\n",
    "# model = CNNAgent([64, 64, 3], 15, channels=16, layers=[1,1,1,1], scale=[1,1,1,1], vheadLayers=1).to(device)\n",
    "model.train()\n",
    "summary(model, input_size=(2, 3, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(modelPath + \"vitNegT8BigFin\" + \"/model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terminateReward -0.25 livingReward -0.001 discountedSumLiving -0.09999999999999991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Miniconda3\\envs\\torch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from PPO import PPO\n",
    "import ProcgenPlayer\n",
    "\n",
    "rewardScale = 8.0\n",
    "terminateReward = 1 - 10.0 / rewardScale\n",
    "livingReward = 0\n",
    "print(\"terminateReward\", terminateReward, \"livingReward\", livingReward, \"discountedSumLiving\", livingReward / (1 - 0.99)) # if terminate reward > discountedSumLiving the agent will perfer to run into obstacles.\n",
    "player = ProcgenPlayer.Player(env, num_agents=num_agents, epsilon=0.01, epsilon_decay=0.99, rewardScale=rewardScale, livingReward=0, terminateReward=terminateReward)\n",
    "ppo = PPO(model, env, num_agents=num_agents, player=player, gamma=0.99, weight_decay=0.0, warmup_steps=10, train_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ppo.runGame()\n",
    "# loss = ppo.train(debug=True)\n",
    "# print(loss)\n",
    "# import torchviz\n",
    "# torchviz.make_dot(loss, params=dict(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadAll(\"vitNegT8BigFin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episodeLength 774.5000 episodeReward -0.1806 epoch 0 steps 4096 loss 0.0029 policy 0.0000 value 0.0029 entropy 2.7044 stale 0              \n",
      "episodeLength 726.8333 episodeReward -0.2500 epoch 10 steps 45056 loss 0.0057 policy 0.0022 value 0.0035 entropy 2.6992 stale 973              \n",
      "episodeLength 563.7500 episodeReward -0.2500 epoch 20 steps 86016 loss -0.0009 policy -0.0017 value 0.0008 entropy 2.6806 stale 994              \n",
      "episodeLength 539.2500 episodeReward 0.0625 epoch 30 steps 126976 loss 0.0050 policy 0.0038 value 0.0012 entropy 2.6727 stale 886               \n",
      "episodeLength 251.0000 episodeReward 0.3333 epoch 40 steps 167936 loss 0.0053 policy 0.0009 value 0.0044 entropy 2.6598 stale 437                \n",
      "episodeLength 172.1351 episodeReward 0.4595 epoch 50 steps 208896 loss 0.0078 policy 0.0000 value 0.0078 entropy 2.5770 stale 320               \n",
      "episodeLength 155.5000 episodeReward 0.7321 epoch 60 steps 249856 loss 0.0079 policy 0.0004 value 0.0075 entropy 2.4823 stale 325               \n",
      "episodeLength 81.0152 episodeReward 0.3750 epoch 70 steps 290816 loss 0.0126 policy 0.0009 value 0.0117 entropy 2.3070 stale 118                \n",
      "episodeLength 91.6471 episodeReward 0.5588 epoch 80 steps 331776 loss 0.0098 policy -0.0002 value 0.0100 entropy 2.3296 stale 198               \n",
      "episodeLength 87.2407 episodeReward 0.4676 epoch 90 steps 372736 loss 0.0124 policy 0.0006 value 0.0117 entropy 2.3486 stale 149               \n",
      "episodeLength 80.5303 episodeReward 0.3371 epoch 100 steps 413696 loss 0.0137 policy 0.0018 value 0.0119 entropy 2.2551 stale 96                \n",
      "episodeLength 63.0500 episodeReward 0.4792 epoch 110 steps 454656 loss 0.0139 policy 0.0004 value 0.0135 entropy 2.1413 stale 131               \n",
      "episodeLength 59.1571 episodeReward 0.5357 epoch 120 steps 495616 loss 0.0159 policy 0.0008 value 0.0151 entropy 1.9992 stale 97                \n",
      "episodeLength 64.9667 episodeReward 0.4583 epoch 130 steps 536576 loss 0.0131 policy -0.0008 value 0.0139 entropy 1.9071 stale 135              \n",
      "episodeLength 1000.0000 episodeReward -0.2500 epoch 140 steps 577536 loss 0.0056 policy 0.0039 value 0.0017 entropy 1.0129 stale 989              \n",
      "episodeLength 0.0000 episodeReward 0.0000 epoch 150 steps 618496 loss -0.0003 policy -0.0003 value 0.0000 entropy 1.0665 stale 556                \n",
      "episodeLength 0.0000 episodeReward 0.0000 epoch 160 steps 659456 loss 0.0003 policy 0.0003 value 0.0000 entropy 0.9068 stale 996                    \n",
      "episodeLength 0.0000 episodeReward 0.0000 epoch 170 steps 700416 loss 0.0004 policy 0.0004 value 0.0000 entropy 0.9479 stale 436                  \n",
      "episodeLength 0.0000 episodeReward 0.0000 epoch 180 steps 741376 loss 0.0002 policy 0.0002 value 0.0000 entropy 0.5383 stale 876                  \n",
      "episodeLength 1000.0000 episodeReward -0.2500 epoch 190 steps 782336 loss 0.0017 policy 0.0015 value 0.0003 entropy 0.4192 stale 316                \n",
      "episodeLength 1000.0000 episodeReward -0.2500 epoch 199 steps 819200 loss 0.0011 policy 0.0011 value 0.0001 entropy 0.3240 stale 994               \r"
     ]
    }
   ],
   "source": [
    "for i in range(200):\n",
    "    ppo.runGame()\n",
    "    ppo.train()\n",
    "    if i % 10 == 0:\n",
    "        # print(\"episodeLength\", ppo.all_stats[-1][\"game/episodeLength\"], \"episodeReward\", ppo.all_stats[-1][\"game/episodeReward\"],\n",
    "        #       \"epoch\", ppo.all_stats[-1][\"epoch\"], \"steps\", ppo.all_stats[-1][\"steps\"], \n",
    "        #       \"\\nloss\", ppo.all_stats[-1][\"ppo/loss/total\"].item(), \"policy\", ppo.all_stats[-1][\"ppo/loss/policy\"].item(), \n",
    "        #       \"value\", ppo.all_stats[-1][\"ppo/loss/value\"].item(),\n",
    "        #       \"entropy\", ppo.all_stats[-1][\"ppo/policy/entropy\"].item())\n",
    "        print(f\"episodeLength {ppo.all_stats[-1]['game/episodeLength']:.4f} episodeReward {ppo.all_stats[-1]['game/episodeReward']:.4f} \" + \n",
    "              f\"epoch {ppo.all_stats[-1]['epoch']} steps {ppo.all_stats[-1]['steps']} \" +\n",
    "              f\"loss {ppo.all_stats[-1]['ppo/loss/total'].item():.4f} policy {ppo.all_stats[-1]['ppo/loss/policy'].item():.4f} \" +\n",
    "              f\"value {ppo.all_stats[-1]['ppo/loss/value'].item():.4f} entropy {ppo.all_stats[-1]['ppo/policy/entropy'].item():.4f} \" +\n",
    "              f\"stale {ppo.all_stats[-1]['game/staleSteps']}              \")\n",
    "    else:\n",
    "        print(f\"episodeLength {ppo.all_stats[-1]['game/episodeLength']:.4f} episodeReward {ppo.all_stats[-1]['game/episodeReward']:.4f} \" + \n",
    "              f\"epoch {ppo.all_stats[-1]['epoch']} steps {ppo.all_stats[-1]['steps']} \" +\n",
    "              f\"loss {ppo.all_stats[-1]['ppo/loss/total'].item():.4f} policy {ppo.all_stats[-1]['ppo/loss/policy'].item():.4f} \" +\n",
    "              f\"value {ppo.all_stats[-1]['ppo/loss/value'].item():.4f} entropy {ppo.all_stats[-1]['ppo/policy/entropy'].item():.4f} \" +\n",
    "              f\"stale {ppo.all_stats[-1]['game/staleSteps']}              \", end=\"\\r\")\n",
    "    # if i % 100 == 0:\n",
    "    #     stats = ppo.all_stats[-1]\n",
    "    #     for k, v in stats.items():\n",
    "    #         # if \"time\" in k:\n",
    "    #         print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveAll(\"vitNegT8BigFin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment Notes\n",
    "\n",
    "1. coin run hard\n",
    "\n",
    "Higher reward per episode is better, and shorter episodes are better. Not sure if reward per timestep consolidates these.\n",
    "The model can learn, and actively persue the reward but get lower reward because it now dies more. Though this is clearly better than headslamming a wall for several hundred  timesteps before randomly jumping into a coin. Shorter episodes also means higher sample size on reward per episode.\n",
    "\n",
    "1.1 VIT 15k\n",
    "\n",
    "LR 1e-3 warmup and cosine decay\n",
    "\n",
    "Weight decay 0.01, living reward -0.001\n",
    "Doesn't like to jump doesn't seem to avoid enemies.\n",
    "\n",
    "Weight decay 0.01\n",
    "Doesn't like to jump\n",
    "\n",
    "No wd or livrew\n",
    "Gets stuck going right\n",
    "\n",
    "Removed Background and theme variation\n",
    "got up to 87% rewards, though most epochs are ~70%\n",
    "Very jumpy, spends almost no time on ground\n",
    "Does not seem to avoid obstacles, always jump appears to be a passive strategy\n",
    "\n",
    "Negative terminate reward. +1 for coin, -1 for enemy or timeout.\n",
    "never got better than -1 reward.\n",
    "\n",
    "Negative terminate reward. +1 for coin, -0.25 for enemy or timeout.\n",
    "Improves for 200 epochs, gets worse around 350. Gets to around 0.5 which is 75% of coins.\n",
    "\n",
    "1.2 VIT 60k\n",
    "2 Layer value head degrades performance, is about 2k extra params. Vf loss gets larger when training.\n",
    "\n",
    "Negative terminate reward. +1 for coin, -0.25 for enemy or timeout.\n",
    "Did well until an over 1k step episode, spiked vf error.\n",
    "\n",
    "Keep running environment on extra steps until every episode finishes.\n",
    "Stale steps started very high around 90, but by epoch 70 it was down to 140. 256 steps per epoch, so a value higher than 256 is an entirely stale epoch.\n",
    "Was considering capping the amount of stale steps, but it seems to work without this after a bit.\n",
    "Reward is steadily increasing.\n",
    "Still very jumpy but not as much as other models. Stays on ground for a few frames. Does not seem to actively avoid obstacles, though appears to be favoring landing on boxes over the ground which are always safe.\n",
    "\n",
    "Add living reward, train from previous model.\n",
    "Discounted Sum of infinite living rewards is less than half the terminate reward so the agent should not perfer to hit an obstacle and end the episode.\n",
    "Reduced high episode lenght faster by epoch 50\n",
    "1000 step episodes at epoch 140 and derailed \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ac488eaa353570522d4c04bd2cd8e3c67c3437ec54aafb89a01cbc7941828458"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
