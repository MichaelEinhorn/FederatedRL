{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from procgen import ProcgenGym3Env\n",
    "from torchinfo import summary\n",
    "import core\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model = None\n",
    "player = None\n",
    "ppo = None\n",
    "env= None\n",
    "envKW = {}\n",
    "\n",
    "modelPath = \"models/\"\n",
    "def loadAll(fname, loadEnv=True):\n",
    "    model.load_state_dict(torch.load(modelPath + fname + \"/model.pth\"))\n",
    "    player.load_state_dict(torch.load(modelPath + fname + \"/player.pth\"))\n",
    "    ppo.load_state_dict(torch.load(modelPath + fname + \"/ppo.pth\"))\n",
    "    if loadEnv:\n",
    "        envKW = torch.load(modelPath + fname + \"/envKW.pth\")\n",
    "        env = ProcgenGym3Env(**envKW)\n",
    "        env.callmethod(\"set_state\", torch.load(modelPath + fname + \"/env_states.pth\"))\n",
    "    else:\n",
    "        player.reset()\n",
    "\n",
    "def saveAll(fname):\n",
    "    import os\n",
    "    os.makedirs(modelPath + fname, exist_ok=True)\n",
    "    torch.save(model.state_dict(), modelPath + fname + \"/model.pth\")\n",
    "    torch.save(player.state_dict(), modelPath + fname + \"/player.pth\")\n",
    "    torch.save(ppo.state_dict(), modelPath + fname + \"/ppo.pth\")\n",
    "    torch.save(envKW, modelPath + fname + \"/envKW.pth\")\n",
    "    torch.save(env.callmethod(\"get_state\"), modelPath + fname + \"/env_states.pth\")\n",
    "    torch.save(ppo.all_stats, modelPath + fname + \"/stats.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict(rgb=D256[64,64,3])\n",
      "D15[]\n"
     ]
    }
   ],
   "source": [
    "num_agents = 16\n",
    "envKW = core.getKW(num=num_agents, env_name=\"coinrun\", distribution_mode=\"easy\", paint_vel_info=True, use_backgrounds=False, restrict_themes=True)\n",
    "env = ProcgenGym3Env(**envKW)\n",
    "print(env.ob_space)\n",
    "print(env.ac_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ViTValue                                 [2, 15]                   --\n",
       "├─VisionTransformer: 1-1                 --                        8,256\n",
       "│    └─PatchEmbed: 2-1                   [2, 256, 32]              --\n",
       "│    │    └─Conv2d: 3-1                  [2, 32, 16, 16]           1,568\n",
       "│    │    └─Identity: 3-2                [2, 256, 32]              --\n",
       "│    └─Dropout: 2-2                      [2, 257, 32]              --\n",
       "│    └─Identity: 2-3                     [2, 257, 32]              --\n",
       "│    └─Sequential: 2-4                   [2, 257, 32]              --\n",
       "│    │    └─Block: 3-3                   [2, 257, 32]              12,704\n",
       "│    │    └─Block: 3-4                   [2, 257, 32]              12,704\n",
       "│    │    └─Block: 3-5                   [2, 257, 32]              12,704\n",
       "│    │    └─Block: 3-6                   [2, 257, 32]              12,704\n",
       "│    └─LayerNorm: 2-5                    [2, 257, 32]              64\n",
       "│    └─Identity: 2-6                     [2, 32]                   --\n",
       "│    └─Linear: 2-7                       [2, 15]                   495\n",
       "├─ValueHead: 1-2                         [2, 1]                    --\n",
       "│    └─Linear: 2-8                       [2, 1]                    33\n",
       "==========================================================================================\n",
       "Total params: 61,232\n",
       "Trainable params: 61,232\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.91\n",
       "==========================================================================================\n",
       "Input size (MB): 0.10\n",
       "Forward/backward pass size (MB): 6.05\n",
       "Params size (MB): 0.21\n",
       "Estimated Total Size (MB): 6.36\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from CVModels import CNNAgent, ViTValue\n",
    "model = ViTValue(depth=4, num_heads=4, embed_dim=32, mlp_ratio=4, valueHeadLayers=1).to(device)\n",
    "# model = ViTValue(depth=3, num_heads=4, embed_dim=16, mlp_ratio=4, valueHeadLayers=1).to(device)\n",
    "# model = CNNAgent([64, 64, 3], 15, channels=16, layers=[1,1,1,1], scale=[1,1,1,1], vheadLayers=1).to(device)\n",
    "model.train()\n",
    "summary(model, input_size=(2, 3, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(modelPath + \"vitNegT8BigFin\" + \"/model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terminateReward -0.25 livingReward 0 discountedSumLiving 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Miniconda3\\envs\\torch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from PPO import PPO\n",
    "import ProcgenPlayer\n",
    "\n",
    "gamma = 0.99\n",
    "# gamma = 0.999\n",
    "rewardScale = 8\n",
    "terminateReward = 1 - 10.0 / rewardScale\n",
    "livingReward = 0\n",
    "# livingReward = -1e-4\n",
    "lr = 1e-3\n",
    "# lr = 5e-4\n",
    "ent_coef = 0\n",
    "# ent_coef = 1e-2\n",
    "print(\"terminateReward\", terminateReward, \"livingReward\", livingReward, \"discountedSumLiving\", livingReward / (1 - gamma)) # if terminate reward > discountedSumLiving the agent will perfer to run into obstacles.\n",
    "player = ProcgenPlayer.Player(env, num_agents=num_agents, epsilon=0.01, epsilon_decay=0.99, rewardScale=rewardScale, livingReward=livingReward, terminateReward=terminateReward, finishedOnly=True, maxStaleSteps=1000)\n",
    "ppo = PPO(model, env, num_agents=num_agents, player=player, lr=lr, gamma=gamma, weight_decay=0.0, ent_coef=ent_coef, warmup_steps=10, train_steps=1000, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ppo.runGame()\n",
    "# loss = ppo.train(debug=True)\n",
    "# print(loss)\n",
    "# import torchviz\n",
    "# torchviz.make_dot(loss, params=dict(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alg_name': 'ppo', 'lr': 0.001, 'gamma': 0.99, 'lam': 0.95, 'whiten': True, 'cliprange': 0.2, 'cliprange_value': 0.2, 'vf_coef': 0.5, 'epoch_steps': 256, 'epochs_per_game': 1, 'ent_coef': 0, 'weight_decay': 0.0, 'warmup_steps': 10, 'train_steps': 1000, 'batch_size': 1}\n",
      "{'alg_name': 'ppo', 'epsilon': 0.01, 'epsilon_decay': 0.99, 'rewardScale': 8, 'livingReward': 0, 'terminateReward': -0.25, 'finishedOnly': True, 'maxStaleSteps': 1000}\n",
      "{'num': 16, 'env_name': 'coinrun', 'distribution_mode': 'easy', 'paint_vel_info': True, 'use_backgrounds': False, 'restrict_themes': True}\n"
     ]
    }
   ],
   "source": [
    "# loadAll(\"vitNegT8BigFin\")\n",
    "print(ppo.params)\n",
    "print(player.params)\n",
    "print(envKW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episodeLength 689.4211 episodeReward -0.1842 epoch 0 steps 4096 loss 0.0005 policy 0.0000 value 0.0005 entropy 2.7026 stale 0              \n",
      "{'epoch': 0, 'steps': 4096, 'objective/vf_coef': 0.5, 'objective/ent_coef': 0, 'objective/lr': [0.0], 'game/episodeLength': 689.421052631579, 'game/nonZeroReward': 0.05263157894736842, 'game/episodeReward': -0.18421052631578946, 'game/advantageMean_PreWhiten': -0.04250799167831403, 'game/advantageStd_PreWhiten': 0.013483489077771077, 'game/staleSteps': 0, 'ppo/reward': tensor(0.), 'ppo/loss/policy': tensor(4.8654e-17, dtype=torch.float64), 'ppo/loss/value': tensor(0.0005, dtype=torch.float64), 'ppo/loss/ent': tensor(-0.), 'ppo/loss/total': tensor(0.0005, dtype=torch.float64), 'ppo/policy/entropy': tensor(2.7026), 'ppo/policy/approxkl': tensor(0.), 'ppo/policy/policykl': tensor(0.), 'ppo/policy/clipfrac': tensor(0., dtype=torch.float64), 'ppo/policy/advantages_mean': tensor(-4.8654e-17, dtype=torch.float64), 'ppo/policy/ratio_mean': tensor(1.), 'ppo/returns/mean': tensor(0.2041, dtype=torch.float64), 'ppo/returns/var': tensor(0.0002, dtype=torch.float64), 'ppo/val/vpred': tensor(0.2468), 'ppo/val/clipfrac': tensor(0., dtype=torch.float64), 'ppo/val/mean': tensor(0.2468), 'ppo/val/var': tensor(9.9025e-05), 'time/runGame': 13.02100157737732, 'time/computeAdvantages': 0.20602917671203613, 'time/ppo/forward': 1.6704473495483398, 'time/ppo/backward': 3.328035593032837, 'time/ppo/optim': 2.6631243228912354, 'time/ppo/stats': 0.7692084312438965, 'time/epoch': 9.196220636367798, 'time/game/observe': 2.540968418121338, 'time/game/act': 0.1119692325592041, 'time/game/forward': 8.415559768676758, 'time/game/stats': 0.9374353885650635, 'time/game/transition': 0.6980276107788086}\n",
      "episodeLength 937.1250 episodeReward 0.2188 epoch 10 steps 45056 loss 0.0038 policy 0.0031 value 0.0007 entropy 2.6828 stale 931                \n",
      "{'epoch': 10, 'steps': 45056, 'objective/vf_coef': 0.5, 'objective/ent_coef': 0, 'objective/lr': [0.001], 'game/episodeLength': 937.125, 'game/nonZeroReward': 0.375, 'game/episodeReward': 0.21875, 'game/advantageMean_PreWhiten': -0.03532283840431887, 'game/advantageStd_PreWhiten': 0.04203686039462921, 'game/staleSteps': 931, 'ppo/reward': tensor(0.0002), 'ppo/loss/policy': tensor(0.0031, dtype=torch.float64), 'ppo/loss/value': tensor(0.0007, dtype=torch.float64), 'ppo/loss/ent': tensor(-0.), 'ppo/loss/total': tensor(0.0038, dtype=torch.float64), 'ppo/policy/entropy': tensor(2.6828), 'ppo/policy/approxkl': tensor(0.0110), 'ppo/policy/policykl': tensor(-0.0096), 'ppo/policy/clipfrac': tensor(0.0913, dtype=torch.float64), 'ppo/policy/advantages_mean': tensor(6.1528e-17, dtype=torch.float64), 'ppo/policy/ratio_mean': tensor(1.0011), 'ppo/returns/mean': tensor(0.1112, dtype=torch.float64), 'ppo/returns/var': tensor(0.0026, dtype=torch.float64), 'ppo/val/vpred': tensor(0.1124), 'ppo/val/clipfrac': tensor(0., dtype=torch.float64), 'ppo/val/mean': tensor(0.1459), 'ppo/val/var': tensor(0.0001), 'time/runGame': 3.8560009002685547, 'time/computeAdvantages': 0.24000000953674316, 'time/ppo/forward': 1.6404740810394287, 'time/ppo/backward': 2.7365736961364746, 'time/ppo/optim': 2.619459867477417, 'time/ppo/stats': 0.7462055683135986, 'time/epoch': 8.409003496170044, 'time/game/observe': 0.4559488296508789, 'time/game/act': 0.035996437072753906, 'time/game/forward': 2.7238893508911133, 'time/game/stats': 0.2961845397949219, 'time/game/transition': 0.2440052032470703}\n",
      "episodeLength 889.4000 episodeReward -0.2500 epoch 20 steps 86016 loss 0.0031 policy 0.0028 value 0.0002 entropy 2.6909 stale 966                 \n",
      "{'epoch': 20, 'steps': 86016, 'objective/vf_coef': 0.5, 'objective/ent_coef': 0, 'objective/lr': [0.0009997482711915926], 'game/episodeLength': 889.4, 'game/nonZeroReward': 0.0, 'game/episodeReward': -0.25, 'game/advantageMean_PreWhiten': -0.019472488994643457, 'game/advantageStd_PreWhiten': 0.029128226928932584, 'game/staleSteps': 966, 'ppo/reward': tensor(-0.0002), 'ppo/loss/policy': tensor(0.0028, dtype=torch.float64), 'ppo/loss/value': tensor(0.0002, dtype=torch.float64), 'ppo/loss/ent': tensor(-0.), 'ppo/loss/total': tensor(0.0031, dtype=torch.float64), 'ppo/policy/entropy': tensor(2.6909), 'ppo/policy/approxkl': tensor(0.0125), 'ppo/policy/policykl': tensor(-0.0162), 'ppo/policy/clipfrac': tensor(0.0879, dtype=torch.float64), 'ppo/policy/advantages_mean': tensor(4.6404e-17, dtype=torch.float64), 'ppo/policy/ratio_mean': tensor(0.9968), 'ppo/returns/mean': tensor(0.0686, dtype=torch.float64), 'ppo/returns/var': tensor(0.0009, dtype=torch.float64), 'ppo/val/vpred': tensor(0.0692), 'ppo/val/clipfrac': tensor(0., dtype=torch.float64), 'ppo/val/mean': tensor(0.0881), 'ppo/val/var': tensor(1.2746e-06), 'time/runGame': 2.215001344680786, 'time/computeAdvantages': 0.20399904251098633, 'time/ppo/forward': 1.7225470542907715, 'time/ppo/backward': 2.8182685375213623, 'time/ppo/optim': 2.721774101257324, 'time/ppo/stats': 0.767723560333252, 'time/epoch': 8.752009868621826, 'time/game/observe': 0.21300482749938965, 'time/game/act': 0.02300429344177246, 'time/game/forward': 1.5949032306671143, 'time/game/stats': 0.18107175827026367, 'time/game/transition': 0.14400768280029297}\n",
      "episodeLength 1000.0000 episodeReward -0.2500 epoch 30 steps 126976 loss 0.0049 policy 0.0047 value 0.0002 entropy 2.6841 stale 939                \n",
      "{'epoch': 30, 'steps': 126976, 'objective/vf_coef': 0.5, 'objective/ent_coef': 0, 'objective/lr': [0.0009989933382359422], 'game/episodeLength': 1000.0, 'game/nonZeroReward': 0.0, 'game/episodeReward': -0.25, 'game/advantageMean_PreWhiten': -0.013086321538511945, 'game/advantageStd_PreWhiten': 0.026683859034317154, 'game/staleSteps': 939, 'ppo/reward': tensor(-0.0002), 'ppo/loss/policy': tensor(0.0047, dtype=torch.float64), 'ppo/loss/value': tensor(0.0002, dtype=torch.float64), 'ppo/loss/ent': tensor(-0.), 'ppo/loss/total': tensor(0.0049, dtype=torch.float64), 'ppo/policy/entropy': tensor(2.6841), 'ppo/policy/approxkl': tensor(0.0076), 'ppo/policy/policykl': tensor(-0.0112), 'ppo/policy/clipfrac': tensor(0.0508, dtype=torch.float64), 'ppo/policy/advantages_mean': tensor(2.8189e-18, dtype=torch.float64), 'ppo/policy/ratio_mean': tensor(0.9963), 'ppo/returns/mean': tensor(0.0376, dtype=torch.float64), 'ppo/returns/var': tensor(0.0007, dtype=torch.float64), 'ppo/val/vpred': tensor(0.0379), 'ppo/val/clipfrac': tensor(0., dtype=torch.float64), 'ppo/val/mean': tensor(0.0507), 'ppo/val/var': tensor(1.1358e-06), 'time/runGame': 2.88900089263916, 'time/computeAdvantages': 0.1979999542236328, 'time/ppo/forward': 1.6453149318695068, 'time/ppo/backward': 2.7546346187591553, 'time/ppo/optim': 2.6042709350585938, 'time/ppo/stats': 0.7436256408691406, 'time/epoch': 8.410996675491333, 'time/game/observe': 0.27497196197509766, 'time/game/act': 0.0250089168548584, 'time/game/forward': 2.0884790420532227, 'time/game/stats': 0.22113943099975586, 'time/game/transition': 0.19698500633239746}\n",
      "episodeLength 1000.0000 episodeReward -0.0417 epoch 40 steps 167936 loss 0.0049 policy 0.0045 value 0.0004 entropy 2.6793 stale 983                \n",
      "{'epoch': 40, 'steps': 167936, 'objective/vf_coef': 0.5, 'objective/ent_coef': 0, 'objective/lr': [0.0009977359612865424], 'game/episodeLength': 1000.0, 'game/nonZeroReward': 0.16666666666666666, 'game/episodeReward': -0.041666666666666664, 'game/advantageMean_PreWhiten': -0.0019728470297368022, 'game/advantageStd_PreWhiten': 0.025676581497037915, 'game/staleSteps': 983, 'ppo/reward': tensor(-6.1035e-05), 'ppo/loss/policy': tensor(0.0045, dtype=torch.float64), 'ppo/loss/value': tensor(0.0004, dtype=torch.float64), 'ppo/loss/ent': tensor(-0.), 'ppo/loss/total': tensor(0.0049, dtype=torch.float64), 'ppo/policy/entropy': tensor(2.6793), 'ppo/policy/approxkl': tensor(0.0125), 'ppo/policy/policykl': tensor(-0.0147), 'ppo/policy/clipfrac': tensor(0.0994, dtype=torch.float64), 'ppo/policy/advantages_mean': tensor(-2.1034e-17, dtype=torch.float64), 'ppo/policy/ratio_mean': tensor(0.9977), 'ppo/returns/mean': tensor(-0.0048, dtype=torch.float64), 'ppo/returns/var': tensor(0.0010, dtype=torch.float64), 'ppo/val/vpred': tensor(-0.0047), 'ppo/val/clipfrac': tensor(0., dtype=torch.float64), 'ppo/val/mean': tensor(-0.0032), 'ppo/val/var': tensor(4.6675e-07), 'time/runGame': 2.9543235301971436, 'time/computeAdvantages': 0.21899914741516113, 'time/ppo/forward': 1.670116901397705, 'time/ppo/backward': 2.7618408203125, 'time/ppo/optim': 2.6905012130737305, 'time/ppo/stats': 0.7339727878570557, 'time/epoch': 8.536606311798096, 'time/game/observe': 0.272005558013916, 'time/game/act': 0.02400350570678711, 'time/game/forward': 2.161757707595825, 'time/game/stats': 0.2151811122894287, 'time/game/transition': 0.20138216018676758}\n",
      "episodeLength 759.2500 episodeReward -0.2500 epoch 50 steps 208896 loss 0.0024 policy 0.0021 value 0.0003 entropy 2.6637 stale 925                 \n",
      "{'epoch': 50, 'steps': 208896, 'objective/vf_coef': 0.5, 'objective/ent_coef': 0, 'objective/lr': [0.0009959774064153978], 'game/episodeLength': 759.25, 'game/nonZeroReward': 0.0, 'game/episodeReward': -0.25, 'game/advantageMean_PreWhiten': -0.008488286195803463, 'game/advantageStd_PreWhiten': 0.026674635485103102, 'game/staleSteps': 925, 'ppo/reward': tensor(0.), 'ppo/loss/policy': tensor(0.0021, dtype=torch.float64), 'ppo/loss/value': tensor(0.0003, dtype=torch.float64), 'ppo/loss/ent': tensor(-0.), 'ppo/loss/total': tensor(0.0024, dtype=torch.float64), 'ppo/policy/entropy': tensor(2.6637), 'ppo/policy/approxkl': tensor(0.0050), 'ppo/policy/policykl': tensor(-0.0030), 'ppo/policy/clipfrac': tensor(0.0430, dtype=torch.float64), 'ppo/policy/advantages_mean': tensor(0., dtype=torch.float64), 'ppo/policy/ratio_mean': tensor(1.0021), 'ppo/returns/mean': tensor(0.0086, dtype=torch.float64), 'ppo/returns/var': tensor(0.0011, dtype=torch.float64), 'ppo/val/vpred': tensor(0.0081), 'ppo/val/clipfrac': tensor(0., dtype=torch.float64), 'ppo/val/mean': tensor(0.0168), 'ppo/val/var': tensor(5.0487e-08), 'time/runGame': 1.495077133178711, 'time/computeAdvantages': 0.18800020217895508, 'time/ppo/forward': 1.685180425643921, 'time/ppo/backward': 2.8228952884674072, 'time/ppo/optim': 2.658586263656616, 'time/ppo/stats': 0.7653205394744873, 'time/epoch': 8.641397953033447, 'time/game/observe': 0.13499784469604492, 'time/game/act': 0.00600433349609375, 'time/game/forward': 1.0910143852233887, 'time/game/stats': 0.11908149719238281, 'time/game/transition': 0.10498499870300293}\n",
      "episodeLength 1000.0000 episodeReward -0.2500 epoch 60 steps 249856 loss -0.0012 policy -0.0013 value 0.0001 entropy 2.6651 stale 969              \n",
      "{'epoch': 60, 'steps': 249856, 'objective/vf_coef': 0.5, 'objective/ent_coef': 0, 'objective/lr': [0.0009937194443381972], 'game/episodeLength': 1000.0, 'game/nonZeroReward': 0.0, 'game/episodeReward': -0.25, 'game/advantageMean_PreWhiten': -0.0010238783395622982, 'game/advantageStd_PreWhiten': 0.015950200499570556, 'game/staleSteps': 969, 'ppo/reward': tensor(-0.0001), 'ppo/loss/policy': tensor(-0.0013, dtype=torch.float64), 'ppo/loss/value': tensor(6.8173e-05, dtype=torch.float64), 'ppo/loss/ent': tensor(-0.), 'ppo/loss/total': tensor(-0.0012, dtype=torch.float64), 'ppo/policy/entropy': tensor(2.6651), 'ppo/policy/approxkl': tensor(0.0074), 'ppo/policy/policykl': tensor(-0.0054), 'ppo/policy/clipfrac': tensor(0.0476, dtype=torch.float64), 'ppo/policy/advantages_mean': tensor(-9.2157e-19, dtype=torch.float64), 'ppo/policy/ratio_mean': tensor(1.0020), 'ppo/returns/mean': tensor(-0.0048, dtype=torch.float64), 'ppo/returns/var': tensor(0.0003, dtype=torch.float64), 'ppo/val/vpred': tensor(-0.0049), 'ppo/val/clipfrac': tensor(0., dtype=torch.float64), 'ppo/val/mean': tensor(-0.0038), 'ppo/val/var': tensor(3.5981e-08), 'time/runGame': 2.9072210788726807, 'time/computeAdvantages': 0.20200133323669434, 'time/ppo/forward': 1.6986896991729736, 'time/ppo/backward': 2.756751537322998, 'time/ppo/optim': 2.6601669788360596, 'time/ppo/stats': 0.7642245292663574, 'time/epoch': 8.554622411727905, 'time/game/observe': 0.27219629287719727, 'time/game/act': 0.01700305938720703, 'time/game/forward': 2.103919267654419, 'time/game/stats': 0.23093867301940918, 'time/game/transition': 0.20315814018249512}\n",
      "episodeLength 776.3333 episodeReward -0.2500 epoch 70 steps 290816 loss 0.0017 policy 0.0016 value 0.0001 entropy 2.6487 stale 942               \n",
      "{'epoch': 70, 'steps': 290816, 'objective/vf_coef': 0.5, 'objective/ent_coef': 0, 'objective/lr': [0.0009909643486313534], 'game/episodeLength': 776.3333333333334, 'game/nonZeroReward': 0.0, 'game/episodeReward': -0.25, 'game/advantageMean_PreWhiten': -0.0027939480321971704, 'game/advantageStd_PreWhiten': 0.02242852861142025, 'game/staleSteps': 942, 'ppo/reward': tensor(-0.0002), 'ppo/loss/policy': tensor(0.0016, dtype=torch.float64), 'ppo/loss/value': tensor(0.0001, dtype=torch.float64), 'ppo/loss/ent': tensor(-0.), 'ppo/loss/total': tensor(0.0017, dtype=torch.float64), 'ppo/policy/entropy': tensor(2.6487), 'ppo/policy/approxkl': tensor(0.0082), 'ppo/policy/policykl': tensor(-0.0094), 'ppo/policy/clipfrac': tensor(0.0630, dtype=torch.float64), 'ppo/policy/advantages_mean': tensor(-1.1926e-18, dtype=torch.float64), 'ppo/policy/ratio_mean': tensor(0.9987), 'ppo/returns/mean': tensor(-0.0076, dtype=torch.float64), 'ppo/returns/var': tensor(0.0005, dtype=torch.float64), 'ppo/val/vpred': tensor(-0.0079), 'ppo/val/clipfrac': tensor(0., dtype=torch.float64), 'ppo/val/mean': tensor(-0.0048), 'ppo/val/var': tensor(3.9066e-08), 'time/runGame': 3.6730048656463623, 'time/computeAdvantages': 0.19399595260620117, 'time/ppo/forward': 1.6795001029968262, 'time/ppo/backward': 2.7216224670410156, 'time/ppo/optim': 2.662142753601074, 'time/ppo/stats': 0.764674186706543, 'time/epoch': 8.542072772979736, 'time/game/observe': 0.36193156242370605, 'time/game/act': 0.028013229370117188, 'time/game/forward': 2.6648707389831543, 'time/game/stats': 0.27011895179748535, 'time/game/transition': 0.234039306640625}\n",
      "episodeLength 1000.0000 episodeReward -0.2500 epoch 80 steps 331776 loss 0.0043 policy 0.0042 value 0.0001 entropy 2.6541 stale 944                \n",
      "{'epoch': 80, 'steps': 331776, 'objective/vf_coef': 0.5, 'objective/ent_coef': 0, 'objective/lr': [0.0009877148934427035], 'game/episodeLength': 1000.0, 'game/nonZeroReward': 0.0, 'game/episodeReward': -0.25, 'game/advantageMean_PreWhiten': -0.0025430414998168674, 'game/advantageStd_PreWhiten': 0.021301157960081425, 'game/staleSteps': 944, 'ppo/reward': tensor(-0.0002), 'ppo/loss/policy': tensor(0.0042, dtype=torch.float64), 'ppo/loss/value': tensor(0.0001, dtype=torch.float64), 'ppo/loss/ent': tensor(-0.), 'ppo/loss/total': tensor(0.0043, dtype=torch.float64), 'ppo/policy/entropy': tensor(2.6541), 'ppo/policy/approxkl': tensor(0.0043), 'ppo/policy/policykl': tensor(-0.0057), 'ppo/policy/clipfrac': tensor(0.0129, dtype=torch.float64), 'ppo/policy/advantages_mean': tensor(2.0979e-17, dtype=torch.float64), 'ppo/policy/ratio_mean': tensor(0.9985), 'ppo/returns/mean': tensor(-0.0089, dtype=torch.float64), 'ppo/returns/var': tensor(0.0004, dtype=torch.float64), 'ppo/val/vpred': tensor(-0.0091), 'ppo/val/clipfrac': tensor(0., dtype=torch.float64), 'ppo/val/mean': tensor(-0.0064), 'ppo/val/var': tensor(2.2930e-09), 'time/runGame': 3.707968235015869, 'time/computeAdvantages': 0.23399996757507324, 'time/ppo/forward': 1.6008670330047607, 'time/ppo/backward': 2.673074960708618, 'time/ppo/optim': 2.5581259727478027, 'time/ppo/stats': 0.7679908275604248, 'time/epoch': 8.302083492279053, 'time/game/observe': 0.32896947860717773, 'time/game/act': 0.03102421760559082, 'time/game/forward': 2.735809326171875, 'time/game/stats': 0.2661139965057373, 'time/game/transition': 0.24303865432739258}\n",
      "episodeLength 950.0000 episodeReward -0.2500 epoch 90 steps 372736 loss 0.0003 policy -0.0000 value 0.0003 entropy 2.6408 stale 822                \n",
      "{'epoch': 90, 'steps': 372736, 'objective/vf_coef': 0.5, 'objective/ent_coef': 0, 'objective/lr': [0.0009839743506981783], 'game/episodeLength': 950.0, 'game/nonZeroReward': 0.0, 'game/episodeReward': -0.25, 'game/advantageMean_PreWhiten': -0.008035516404477836, 'game/advantageStd_PreWhiten': 0.03178609517935886, 'game/staleSteps': 822, 'ppo/reward': tensor(-0.0005), 'ppo/loss/policy': tensor(-6.0805e-07, dtype=torch.float64), 'ppo/loss/value': tensor(0.0003, dtype=torch.float64), 'ppo/loss/ent': tensor(-0.), 'ppo/loss/total': tensor(0.0003, dtype=torch.float64), 'ppo/policy/entropy': tensor(2.6408), 'ppo/policy/approxkl': tensor(0.0097), 'ppo/policy/policykl': tensor(-0.0107), 'ppo/policy/clipfrac': tensor(0.0535, dtype=torch.float64), 'ppo/policy/advantages_mean': tensor(-4.1742e-18, dtype=torch.float64), 'ppo/policy/ratio_mean': tensor(0.9988), 'ppo/returns/mean': tensor(-0.0188, dtype=torch.float64), 'ppo/returns/var': tensor(0.0010, dtype=torch.float64), 'ppo/val/vpred': tensor(-0.0185), 'ppo/val/clipfrac': tensor(0., dtype=torch.float64), 'ppo/val/mean': tensor(-0.0108), 'ppo/val/var': tensor(1.4349e-08), 'time/runGame': 4.793952226638794, 'time/computeAdvantages': 0.21595311164855957, 'time/ppo/forward': 1.7132129669189453, 'time/ppo/backward': 2.860490322113037, 'time/ppo/optim': 2.6809775829315186, 'time/ppo/stats': 0.7729477882385254, 'time/epoch': 8.712159395217896, 'time/game/observe': 0.41594505310058594, 'time/game/act': 0.03901195526123047, 'time/game/forward': 3.5408713817596436, 'time/game/stats': 0.35591864585876465, 'time/game/transition': 0.30117154121398926}\n",
      "episodeLength 479.3000 episodeReward 0.0000 epoch 100 steps 413696 loss 0.0166 policy 0.0164 value 0.0002 entropy 2.6629 stale 989                 \n",
      "{'epoch': 100, 'steps': 413696, 'objective/vf_coef': 0.5, 'objective/ent_coef': 0, 'objective/lr': [0.0009797464868072487], 'game/episodeLength': 479.3, 'game/nonZeroReward': 0.2, 'game/episodeReward': 0.0, 'game/advantageMean_PreWhiten': 0.0029155316566097105, 'game/advantageStd_PreWhiten': 0.01933250719939222, 'game/staleSteps': 989, 'ppo/reward': tensor(-0.0002), 'ppo/loss/policy': tensor(0.0164, dtype=torch.float64), 'ppo/loss/value': tensor(0.0002, dtype=torch.float64), 'ppo/loss/ent': tensor(-0.), 'ppo/loss/total': tensor(0.0166, dtype=torch.float64), 'ppo/policy/entropy': tensor(2.6629), 'ppo/policy/approxkl': tensor(0.0629), 'ppo/policy/policykl': tensor(-0.0614), 'ppo/policy/clipfrac': tensor(0.1958, dtype=torch.float64), 'ppo/policy/advantages_mean': tensor(-1.6209e-17, dtype=torch.float64), 'ppo/policy/ratio_mean': tensor(0.9940), 'ppo/returns/mean': tensor(-0.0278, dtype=torch.float64), 'ppo/returns/var': tensor(0.0004, dtype=torch.float64), 'ppo/val/vpred': tensor(-0.0271), 'ppo/val/clipfrac': tensor(0., dtype=torch.float64), 'ppo/val/mean': tensor(-0.0307), 'ppo/val/var': tensor(3.1288e-09), 'time/runGame': 2.3339684009552, 'time/computeAdvantages': 0.19699978828430176, 'time/ppo/forward': 1.693425178527832, 'time/ppo/backward': 2.815174102783203, 'time/ppo/optim': 2.6880147457122803, 'time/ppo/stats': 0.7616636753082275, 'time/epoch': 8.667925119400024, 'time/game/observe': 0.19196605682373047, 'time/game/act': 0.012996435165405273, 'time/game/forward': 1.723853588104248, 'time/game/stats': 0.16409873962402344, 'time/game/transition': 0.14401483535766602}\n",
      "episodeLength 1000.0000 episodeReward -0.2500 epoch 110 steps 454656 loss -0.0001 policy -0.0002 value 0.0001 entropy 2.6171 stale 993               \n",
      "{'epoch': 110, 'steps': 454656, 'objective/vf_coef': 0.5, 'objective/ent_coef': 0, 'objective/lr': [0.0009750355588704727], 'game/episodeLength': 1000.0, 'game/nonZeroReward': 0.0, 'game/episodeReward': -0.25, 'game/advantageMean_PreWhiten': -0.0013592739950437577, 'game/advantageStd_PreWhiten': 0.015985618872800412, 'game/staleSteps': 993, 'ppo/reward': tensor(-0.0001), 'ppo/loss/policy': tensor(-0.0002, dtype=torch.float64), 'ppo/loss/value': tensor(6.4751e-05, dtype=torch.float64), 'ppo/loss/ent': tensor(-0.), 'ppo/loss/total': tensor(-0.0001, dtype=torch.float64), 'ppo/policy/entropy': tensor(2.6171), 'ppo/policy/approxkl': tensor(0.0119), 'ppo/policy/policykl': tensor(-0.0100), 'ppo/policy/clipfrac': tensor(0.1147, dtype=torch.float64), 'ppo/policy/advantages_mean': tensor(1.0192e-17, dtype=torch.float64), 'ppo/policy/ratio_mean': tensor(1.0017), 'ppo/returns/mean': tensor(-0.0059, dtype=torch.float64), 'ppo/returns/var': tensor(0.0003, dtype=torch.float64), 'ppo/val/vpred': tensor(-0.0062), 'ppo/val/clipfrac': tensor(0., dtype=torch.float64), 'ppo/val/mean': tensor(-0.0046), 'ppo/val/var': tensor(2.2568e-06), 'time/runGame': 1.5139997005462646, 'time/computeAdvantages': 0.2019977569580078, 'time/ppo/forward': 1.6507561206817627, 'time/ppo/backward': 2.692713499069214, 'time/ppo/optim': 2.6318602561950684, 'time/ppo/stats': 0.7477023601531982, 'time/epoch': 8.41572093963623, 'time/game/observe': 0.13196086883544922, 'time/game/act': 0.01699972152709961, 'time/game/forward': 1.1149499416351318, 'time/game/stats': 0.10675883293151855, 'time/game/transition': 0.09701251983642578}\n",
      "episodeLength 1000.0000 episodeReward -0.2500 epoch 120 steps 495616 loss 0.0018 policy 0.0016 value 0.0003 entropy 2.6131 stale 962                \n",
      "{'epoch': 120, 'steps': 495616, 'objective/vf_coef': 0.5, 'objective/ent_coef': 0, 'objective/lr': [0.0009698463103929542], 'game/episodeLength': 1000.0, 'game/nonZeroReward': 0.0, 'game/episodeReward': -0.25, 'game/advantageMean_PreWhiten': -0.007806494990998719, 'game/advantageStd_PreWhiten': 0.033978201693210794, 'game/staleSteps': 962, 'ppo/reward': tensor(-0.0006), 'ppo/loss/policy': tensor(0.0016, dtype=torch.float64), 'ppo/loss/value': tensor(0.0003, dtype=torch.float64), 'ppo/loss/ent': tensor(-0.), 'ppo/loss/total': tensor(0.0018, dtype=torch.float64), 'ppo/policy/entropy': tensor(2.6131), 'ppo/policy/approxkl': tensor(0.0080), 'ppo/policy/policykl': tensor(-0.0055), 'ppo/policy/clipfrac': tensor(0.0723, dtype=torch.float64), 'ppo/policy/advantages_mean': tensor(-2.6888e-17, dtype=torch.float64), 'ppo/policy/ratio_mean': tensor(1.0025), 'ppo/returns/mean': tensor(-0.0203, dtype=torch.float64), 'ppo/returns/var': tensor(0.0011, dtype=torch.float64), 'ppo/val/vpred': tensor(-0.0205), 'ppo/val/clipfrac': tensor(0., dtype=torch.float64), 'ppo/val/mean': tensor(-0.0125), 'ppo/val/var': tensor(5.5889e-07), 'time/runGame': 2.1650028228759766, 'time/computeAdvantages': 0.25899600982666016, 'time/ppo/forward': 1.7286884784698486, 'time/ppo/backward': 2.842599391937256, 'time/ppo/optim': 2.702892303466797, 'time/ppo/stats': 0.7567596435546875, 'time/epoch': 8.735700845718384, 'time/game/observe': 0.20834660530090332, 'time/game/act': 0.020000457763671875, 'time/game/forward': 1.5654761791229248, 'time/game/stats': 0.16407465934753418, 'time/game/transition': 0.14206171035766602}\n",
      "episodeLength 1000.0000 episodeReward -0.2500 epoch 130 steps 536576 loss 0.0053 policy 0.0052 value 0.0001 entropy 2.6215 stale 996                \n",
      "{'epoch': 130, 'steps': 536576, 'objective/vf_coef': 0.5, 'objective/ent_coef': 0, 'objective/lr': [0.0009641839665080363], 'game/episodeLength': 1000.0, 'game/nonZeroReward': 0.0, 'game/episodeReward': -0.25, 'game/advantageMean_PreWhiten': 0.0004942222987611112, 'game/advantageStd_PreWhiten': 0.015575418269101747, 'game/staleSteps': 996, 'ppo/reward': tensor(-0.0001), 'ppo/loss/policy': tensor(0.0052, dtype=torch.float64), 'ppo/loss/value': tensor(6.0218e-05, dtype=torch.float64), 'ppo/loss/ent': tensor(-0.), 'ppo/loss/total': tensor(0.0053, dtype=torch.float64), 'ppo/policy/entropy': tensor(2.6215), 'ppo/policy/approxkl': tensor(0.0143), 'ppo/policy/policykl': tensor(-0.0114), 'ppo/policy/clipfrac': tensor(0.1021, dtype=torch.float64), 'ppo/policy/advantages_mean': tensor(-1.5450e-17, dtype=torch.float64), 'ppo/policy/ratio_mean': tensor(1.0029), 'ppo/returns/mean': tensor(-0.0157, dtype=torch.float64), 'ppo/returns/var': tensor(0.0002, dtype=torch.float64), 'ppo/val/vpred': tensor(-0.0158), 'ppo/val/clipfrac': tensor(0., dtype=torch.float64), 'ppo/val/mean': tensor(-0.0162), 'ppo/val/var': tensor(3.0166e-08), 'time/runGame': 1.0169999599456787, 'time/computeAdvantages': 0.18703126907348633, 'time/ppo/forward': 1.6560771465301514, 'time/ppo/backward': 2.753551721572876, 'time/ppo/optim': 2.6331968307495117, 'time/ppo/stats': 0.7456800937652588, 'time/epoch': 8.47791314125061, 'time/game/observe': 0.07899951934814453, 'time/game/act': 0.01500248908996582, 'time/game/forward': 0.7569565773010254, 'time/game/stats': 0.07000732421875, 'time/game/transition': 0.06502985954284668}\n",
      "episodeLength 1000.0000 episodeReward -0.2500 epoch 140 steps 577536 loss 0.0007 policy 0.0006 value 0.0001 entropy 2.6425 stale 991                \n",
      "{'epoch': 140, 'steps': 577536, 'objective/vf_coef': 0.5, 'objective/ent_coef': 0, 'objective/lr': [0.0009580542287160348], 'game/episodeLength': 1000.0, 'game/nonZeroReward': 0.0, 'game/episodeReward': -0.25, 'game/advantageMean_PreWhiten': 0.0031683977766001565, 'game/advantageStd_PreWhiten': 0.0172510406106788, 'game/staleSteps': 991, 'ppo/reward': tensor(-0.0002), 'ppo/loss/policy': tensor(0.0006, dtype=torch.float64), 'ppo/loss/value': tensor(8.9974e-05, dtype=torch.float64), 'ppo/loss/ent': tensor(-0.), 'ppo/loss/total': tensor(0.0007, dtype=torch.float64), 'ppo/policy/entropy': tensor(2.6425), 'ppo/policy/approxkl': tensor(0.0070), 'ppo/policy/policykl': tensor(-0.0090), 'ppo/policy/clipfrac': tensor(0.0400, dtype=torch.float64), 'ppo/policy/advantages_mean': tensor(-2.9382e-17, dtype=torch.float64), 'ppo/policy/ratio_mean': tensor(0.9979), 'ppo/returns/mean': tensor(-0.0266, dtype=torch.float64), 'ppo/returns/var': tensor(0.0003, dtype=torch.float64), 'ppo/val/vpred': tensor(-0.0266), 'ppo/val/clipfrac': tensor(0., dtype=torch.float64), 'ppo/val/mean': tensor(-0.0298), 'ppo/val/var': tensor(1.0085e-08), 'time/runGame': 1.7400133609771729, 'time/computeAdvantages': 0.19936013221740723, 'time/ppo/forward': 1.7175726890563965, 'time/ppo/backward': 2.81880521774292, 'time/ppo/optim': 2.737457752227783, 'time/ppo/stats': 0.7778835296630859, 'time/epoch': 8.749528646469116, 'time/game/observe': 0.15497660636901855, 'time/game/act': 0.013003349304199219, 'time/game/forward': 1.2689368724822998, 'time/game/stats': 0.1301262378692627, 'time/game/transition': 0.11995840072631836}\n",
      "episodeLength 0.0000 episodeReward 0.0000 epoch 150 steps 618496 loss -0.0034 policy -0.0034 value 0.0000 entropy 2.6358 stale 965                  \n",
      "{'epoch': 150, 'steps': 618496, 'objective/vf_coef': 0.5, 'objective/ent_coef': 0, 'objective/lr': [0.0009514632691433108], 'game/episodeLength': 0, 'game/nonZeroReward': 0, 'game/episodeReward': 0, 'game/advantageMean_PreWhiten': 0.0020845941549822804, 'game/advantageStd_PreWhiten': 0.0023653409121463654, 'game/staleSteps': 965, 'ppo/reward': tensor(0.), 'ppo/loss/policy': tensor(-0.0034, dtype=torch.float64), 'ppo/loss/value': tensor(2.2512e-06, dtype=torch.float64), 'ppo/loss/ent': tensor(-0.), 'ppo/loss/total': tensor(-0.0034, dtype=torch.float64), 'ppo/policy/entropy': tensor(2.6358), 'ppo/policy/approxkl': tensor(0.0122), 'ppo/policy/policykl': tensor(-0.0130), 'ppo/policy/clipfrac': tensor(0.1650, dtype=torch.float64), 'ppo/policy/advantages_mean': tensor(-6.9768e-17, dtype=torch.float64), 'ppo/policy/ratio_mean': tensor(0.9990), 'ppo/returns/mean': tensor(-0.0118, dtype=torch.float64), 'ppo/returns/var': tensor(5.5064e-06, dtype=torch.float64), 'ppo/val/vpred': tensor(-0.0120), 'ppo/val/clipfrac': tensor(0., dtype=torch.float64), 'ppo/val/mean': tensor(-0.0139), 'ppo/val/var': tensor(1.1619e-09), 'time/runGame': 0.0, 'time/computeAdvantages': 0.13003110885620117, 'time/ppo/forward': 1.6186745166778564, 'time/ppo/backward': 2.6833863258361816, 'time/ppo/optim': 2.6327757835388184, 'time/ppo/stats': 0.7785577774047852, 'time/epoch': 8.388705730438232, 'time/game/observe': 0, 'time/game/act': 0, 'time/game/forward': 0, 'time/game/stats': 0, 'time/game/transition': 0}\n",
      "episodeLength 1000.0000 episodeReward 0.1667 epoch 160 steps 659456 loss 0.0011 policy 0.0009 value 0.0002 entropy 2.6351 stale 911                 \n",
      "{'epoch': 160, 'steps': 659456, 'objective/vf_coef': 0.5, 'objective/ent_coef': 0, 'objective/lr': [0.0009444177243274617], 'game/episodeLength': 1000.0, 'game/nonZeroReward': 0.3333333333333333, 'game/episodeReward': 0.16666666666666666, 'game/advantageMean_PreWhiten': 0.0007954074106558091, 'game/advantageStd_PreWhiten': 0.018057764119588343, 'game/staleSteps': 911, 'ppo/reward': tensor(0.0001), 'ppo/loss/policy': tensor(0.0009, dtype=torch.float64), 'ppo/loss/value': tensor(0.0002, dtype=torch.float64), 'ppo/loss/ent': tensor(-0.), 'ppo/loss/total': tensor(0.0011, dtype=torch.float64), 'ppo/policy/entropy': tensor(2.6351), 'ppo/policy/approxkl': tensor(0.0063), 'ppo/policy/policykl': tensor(-0.0058), 'ppo/policy/clipfrac': tensor(0.0469, dtype=torch.float64), 'ppo/policy/advantages_mean': tensor(7.8605e-18, dtype=torch.float64), 'ppo/policy/ratio_mean': tensor(1.0005), 'ppo/returns/mean': tensor(-0.0195, dtype=torch.float64), 'ppo/returns/var': tensor(0.0007, dtype=torch.float64), 'ppo/val/vpred': tensor(-0.0196), 'ppo/val/clipfrac': tensor(0., dtype=torch.float64), 'ppo/val/mean': tensor(-0.0206), 'ppo/val/var': tensor(4.6234e-09), 'time/runGame': 3.2860000133514404, 'time/computeAdvantages': 0.225999116897583, 'time/ppo/forward': 1.677152395248413, 'time/ppo/backward': 2.784353017807007, 'time/ppo/optim': 2.6001012325286865, 'time/ppo/stats': 0.7738120555877686, 'time/epoch': 8.564999341964722, 'time/game/observe': 0.2820148468017578, 'time/game/act': 0.02302241325378418, 'time/game/forward': 2.420825481414795, 'time/game/stats': 0.250518798828125, 'time/game/transition': 0.19999003410339355}\n",
      "episodeLength 1000.0000 episodeReward -0.2500 epoch 170 steps 700416 loss 0.0006 policy 0.0004 value 0.0001 entropy 2.6494 stale 845                \n",
      "{'epoch': 170, 'steps': 700416, 'objective/vf_coef': 0.5, 'objective/ent_coef': 0, 'objective/lr': [0.0009369246885348925], 'game/episodeLength': 1000.0, 'game/nonZeroReward': 0.0, 'game/episodeReward': -0.25, 'game/advantageMean_PreWhiten': -0.0018463676261155967, 'game/advantageStd_PreWhiten': 0.02385303341354039, 'game/staleSteps': 845, 'ppo/reward': tensor(-0.0003), 'ppo/loss/policy': tensor(0.0004, dtype=torch.float64), 'ppo/loss/value': tensor(0.0001, dtype=torch.float64), 'ppo/loss/ent': tensor(-0.), 'ppo/loss/total': tensor(0.0006, dtype=torch.float64), 'ppo/policy/entropy': tensor(2.6494), 'ppo/policy/approxkl': tensor(0.0043), 'ppo/policy/policykl': tensor(-0.0034), 'ppo/policy/clipfrac': tensor(0.0042, dtype=torch.float64), 'ppo/policy/advantages_mean': tensor(-5.4210e-18, dtype=torch.float64), 'ppo/policy/ratio_mean': tensor(1.0008), 'ppo/returns/mean': tensor(-0.0228, dtype=torch.float64), 'ppo/returns/var': tensor(0.0006, dtype=torch.float64), 'ppo/val/vpred': tensor(-0.0232), 'ppo/val/clipfrac': tensor(0., dtype=torch.float64), 'ppo/val/mean': tensor(-0.0209), 'ppo/val/var': tensor(2.0086e-09), 'time/runGame': 4.96699857711792, 'time/computeAdvantages': 0.27499818801879883, 'time/ppo/forward': 1.7035741806030273, 'time/ppo/backward': 2.792116641998291, 'time/ppo/optim': 2.6481289863586426, 'time/ppo/stats': 0.8011155128479004, 'time/epoch': 8.652221441268921, 'time/game/observe': 0.46993494033813477, 'time/game/act': 0.04700946807861328, 'time/game/forward': 3.624979257583618, 'time/game/stats': 0.36318254470825195, 'time/game/transition': 0.31603550910949707}\n",
      "episodeLength 1000.0000 episodeReward -0.2500 epoch 180 steps 741376 loss 0.0004 policy 0.0003 value 0.0001 entropy 2.5950 stale 997                \n",
      "{'epoch': 180, 'steps': 741376, 'objective/vf_coef': 0.5, 'objective/ent_coef': 0, 'objective/lr': [0.0009289917066174886], 'game/episodeLength': 1000.0, 'game/nonZeroReward': 0.0, 'game/episodeReward': -0.25, 'game/advantageMean_PreWhiten': -0.0005772303180552232, 'game/advantageStd_PreWhiten': 0.02127059931077641, 'game/staleSteps': 997, 'ppo/reward': tensor(-0.0002), 'ppo/loss/policy': tensor(0.0003, dtype=torch.float64), 'ppo/loss/value': tensor(0.0001, dtype=torch.float64), 'ppo/loss/ent': tensor(-0.), 'ppo/loss/total': tensor(0.0004, dtype=torch.float64), 'ppo/policy/entropy': tensor(2.5950), 'ppo/policy/approxkl': tensor(0.0084), 'ppo/policy/policykl': tensor(-0.0077), 'ppo/policy/clipfrac': tensor(0.0513, dtype=torch.float64), 'ppo/policy/advantages_mean': tensor(4.2284e-18, dtype=torch.float64), 'ppo/policy/ratio_mean': tensor(1.0005), 'ppo/returns/mean': tensor(-0.0219, dtype=torch.float64), 'ppo/returns/var': tensor(0.0004, dtype=torch.float64), 'ppo/val/vpred': tensor(-0.0216), 'ppo/val/clipfrac': tensor(0., dtype=torch.float64), 'ppo/val/mean': tensor(-0.0213), 'ppo/val/var': tensor(3.5562e-06), 'time/runGame': 2.3200151920318604, 'time/computeAdvantages': 0.20400142669677734, 'time/ppo/forward': 1.6848347187042236, 'time/ppo/backward': 2.805424213409424, 'time/ppo/optim': 2.6138529777526855, 'time/ppo/stats': 0.7476761341094971, 'time/epoch': 8.564605712890625, 'time/game/observe': 0.19332122802734375, 'time/game/act': 0.017003774642944336, 'time/game/forward': 1.7203636169433594, 'time/game/stats': 0.17998194694519043, 'time/game/transition': 0.13829374313354492}\n",
      "episodeLength 1000.0000 episodeReward -0.2500 epoch 187 steps 770048 loss 0.0003 policy 0.0003 value 0.0000 entropy 2.6367 stale 990                \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Michael Einhorn\\Documents\\GTML\\RL\\procgen.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Michael%20Einhorn/Documents/GTML/RL/procgen.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m ppo\u001b[39m.\u001b[39mrunGame()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Michael%20Einhorn/Documents/GTML/RL/procgen.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Michael%20Einhorn/Documents/GTML/RL/procgen.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     ppo\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Michael%20Einhorn/Documents/GTML/RL/procgen.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Michael%20Einhorn/Documents/GTML/RL/procgen.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m# print(\"episodeLength\", ppo.all_stats[-1][\"game/episodeLength\"], \"episodeReward\", ppo.all_stats[-1][\"game/episodeReward\"],\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Michael%20Einhorn/Documents/GTML/RL/procgen.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m#       \"epoch\", ppo.all_stats[-1][\"epoch\"], \"steps\", ppo.all_stats[-1][\"steps\"], \u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Michael%20Einhorn/Documents/GTML/RL/procgen.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m#       \"\\nloss\", ppo.all_stats[-1][\"ppo/loss/total\"].item(), \"policy\", ppo.all_stats[-1][\"ppo/loss/policy\"].item(), \u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Michael%20Einhorn/Documents/GTML/RL/procgen.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m#       \"value\", ppo.all_stats[-1][\"ppo/loss/value\"].item(),\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Michael%20Einhorn/Documents/GTML/RL/procgen.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m#       \"entropy\", ppo.all_stats[-1][\"ppo/policy/entropy\"].item())\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Michael%20Einhorn/Documents/GTML/RL/procgen.ipynb#X10sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mepisodeLength \u001b[39m\u001b[39m{\u001b[39;00mppo\u001b[39m.\u001b[39mall_stats[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mgame/episodeLength\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m episodeReward \u001b[39m\u001b[39m{\u001b[39;00mppo\u001b[39m.\u001b[39mall_stats[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mgame/episodeReward\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Michael%20Einhorn/Documents/GTML/RL/procgen.ipynb#X10sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m           \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mepoch \u001b[39m\u001b[39m{\u001b[39;00mppo\u001b[39m.\u001b[39mall_stats[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m steps \u001b[39m\u001b[39m{\u001b[39;00mppo\u001b[39m.\u001b[39mall_stats[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39msteps\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Michael%20Einhorn/Documents/GTML/RL/procgen.ipynb#X10sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m           \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mloss \u001b[39m\u001b[39m{\u001b[39;00mppo\u001b[39m.\u001b[39mall_stats[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mppo/loss/total\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mitem()\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m policy \u001b[39m\u001b[39m{\u001b[39;00mppo\u001b[39m.\u001b[39mall_stats[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mppo/loss/policy\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mitem()\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Michael%20Einhorn/Documents/GTML/RL/procgen.ipynb#X10sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m           \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mvalue \u001b[39m\u001b[39m{\u001b[39;00mppo\u001b[39m.\u001b[39mall_stats[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mppo/loss/value\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mitem()\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m entropy \u001b[39m\u001b[39m{\u001b[39;00mppo\u001b[39m.\u001b[39mall_stats[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mppo/policy/entropy\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mitem()\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Michael%20Einhorn/Documents/GTML/RL/procgen.ipynb#X10sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m           \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstale \u001b[39m\u001b[39m{\u001b[39;00mppo\u001b[39m.\u001b[39mall_stats[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mgame/staleSteps\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m              \u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Michael Einhorn\\Documents\\GTML\\RL\\PPO.py:153\u001b[0m, in \u001b[0;36mPPO.train\u001b[1;34m(self, debug)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtiming[\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtime/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39malg_name\u001b[39m}\u001b[39;00m\u001b[39m/backward\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t\n\u001b[0;32m    152\u001b[0m     t \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m--> 153\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m    154\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtiming[\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtime/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39malg_name\u001b[39m}\u001b[39;00m\u001b[39m/optim\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t\n\u001b[0;32m    156\u001b[0m t \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\envs\\torch\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:65\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     64\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\envs\\torch\\lib\\site-packages\\torch\\optim\\optimizer.py:109\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    107\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m    108\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 109\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\envs\\torch\\lib\\site-packages\\transformers\\optimization.py:360\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    356\u001b[0m state[\u001b[39m\"\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    358\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m    359\u001b[0m \u001b[39m# In-place operations to update the averages at the same time\u001b[39;00m\n\u001b[1;32m--> 360\u001b[0m exp_avg\u001b[39m.\u001b[39;49mmul_(beta1)\u001b[39m.\u001b[39;49madd_(grad, alpha\u001b[39m=\u001b[39;49m(\u001b[39m1.0\u001b[39;49m \u001b[39m-\u001b[39;49m beta1))\n\u001b[0;32m    361\u001b[0m exp_avg_sq\u001b[39m.\u001b[39mmul_(beta2)\u001b[39m.\u001b[39maddcmul_(grad, grad, value\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m \u001b[39m-\u001b[39m beta2)\n\u001b[0;32m    362\u001b[0m denom \u001b[39m=\u001b[39m exp_avg_sq\u001b[39m.\u001b[39msqrt()\u001b[39m.\u001b[39madd_(group[\u001b[39m\"\u001b[39m\u001b[39meps\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(200):\n",
    "    ppo.runGame()\n",
    "    for _ in range(1):\n",
    "        ppo.train()\n",
    "    if i % 10 == 0:\n",
    "        # print(\"episodeLength\", ppo.all_stats[-1][\"game/episodeLength\"], \"episodeReward\", ppo.all_stats[-1][\"game/episodeReward\"],\n",
    "        #       \"epoch\", ppo.all_stats[-1][\"epoch\"], \"steps\", ppo.all_stats[-1][\"steps\"], \n",
    "        #       \"\\nloss\", ppo.all_stats[-1][\"ppo/loss/total\"].item(), \"policy\", ppo.all_stats[-1][\"ppo/loss/policy\"].item(), \n",
    "        #       \"value\", ppo.all_stats[-1][\"ppo/loss/value\"].item(),\n",
    "        #       \"entropy\", ppo.all_stats[-1][\"ppo/policy/entropy\"].item())\n",
    "        print(f\"episodeLength {ppo.all_stats[-1]['game/episodeLength']:.4f} episodeReward {ppo.all_stats[-1]['game/episodeReward']:.4f} \" + \n",
    "              f\"epoch {ppo.all_stats[-1]['epoch']} steps {ppo.all_stats[-1]['steps']} \" +\n",
    "              f\"loss {ppo.all_stats[-1]['ppo/loss/total'].item():.4f} policy {ppo.all_stats[-1]['ppo/loss/policy'].item():.4f} \" +\n",
    "              f\"value {ppo.all_stats[-1]['ppo/loss/value'].item():.4f} entropy {ppo.all_stats[-1]['ppo/policy/entropy'].item():.4f} \" +\n",
    "              f\"stale {ppo.all_stats[-1]['game/staleSteps']}              \")\n",
    "        # print(ppo.all_stats[-1])\n",
    "    else:\n",
    "        print(f\"episodeLength {ppo.all_stats[-1]['game/episodeLength']:.4f} episodeReward {ppo.all_stats[-1]['game/episodeReward']:.4f} \" + \n",
    "              f\"epoch {ppo.all_stats[-1]['epoch']} steps {ppo.all_stats[-1]['steps']} \" +\n",
    "              f\"loss {ppo.all_stats[-1]['ppo/loss/total'].item():.4f} policy {ppo.all_stats[-1]['ppo/loss/policy'].item():.4f} \" +\n",
    "              f\"value {ppo.all_stats[-1]['ppo/loss/value'].item():.4f} entropy {ppo.all_stats[-1]['ppo/policy/entropy'].item():.4f} \" +\n",
    "              f\"stale {ppo.all_stats[-1]['game/staleSteps']}              \", end=\"\\r\")\n",
    "    # if i % 100 == 0:\n",
    "    #     stats = ppo.all_stats[-1]\n",
    "    #     for k, v in stats.items():\n",
    "    #         # if \"time\" in k:\n",
    "    #         print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saveAll(\"vitNegT8BigFin400\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment Notes\n",
    "\n",
    "1. coin run hard\n",
    "\n",
    "Higher reward per episode is better, and shorter episodes are better. Not sure if reward per timestep consolidates these.\n",
    "The model can learn, and actively persue the reward but get lower reward because it now dies more. Though this is clearly better than headslamming a wall for several hundred  timesteps before randomly jumping into a coin. Shorter episodes also means higher sample size on reward per episode.\n",
    "\n",
    "1.1 VIT 15k\n",
    "\n",
    "LR 1e-3 warmup and cosine decay\n",
    "\n",
    "Weight decay 0.01, living reward -0.001\n",
    "Doesn't like to jump doesn't seem to avoid enemies.\n",
    "\n",
    "Weight decay 0.01\n",
    "Doesn't like to jump\n",
    "\n",
    "No wd or livrew\n",
    "Gets stuck going right\n",
    "\n",
    "Removed Background and theme variation\n",
    "got up to 87% rewards, though most epochs are ~70%\n",
    "Very jumpy, spends almost no time on ground\n",
    "Does not seem to avoid obstacles, always jump appears to be a passive strategy\n",
    "\n",
    "Negative terminate reward. +1 for coin, -1 for enemy or timeout.\n",
    "never got better than -1 reward.\n",
    "\n",
    "Negative terminate reward. +1 for coin, -0.25 for enemy or timeout.\n",
    "Improves for 200 epochs, gets worse around 350. Gets to around 0.5 which is 75% of coins.\n",
    "\n",
    "1.2 VIT 60k\n",
    "2 Layer value head degrades performance, is about 2k extra params. Vf loss gets larger when training.\n",
    "\n",
    "Negative terminate reward. +1 for coin, -0.25 for enemy or timeout.\n",
    "Did well until an over 1k step episode, spiked vf error.\n",
    "\n",
    "Keep running environment on extra steps until every episode finishes.\n",
    "Stale steps started very high around 90, but by epoch 70 it was down to 140. 256 steps per epoch, so a value higher than 256 is an entirely stale epoch.\n",
    "Was considering capping the amount of stale steps, but it seems to work without this after a bit.\n",
    "Reward is steadily increasing.\n",
    "Still very jumpy but not as much as other models. Stays on ground for a few frames. Does not seem to actively avoid obstacles, though appears to be favoring landing on boxes over the ground which are always safe.\n",
    "400 epochs, doesn't favor boxes as much, might be a bit better at jumping over gaps and avoiding obstacles.\n",
    "\n",
    "Add living reward, train from previous model.\n",
    "Discounted Sum of infinite living rewards is less than half the terminate reward so the agent should not perfer to hit an obstacle and end the episode.\n",
    "Reduced high episode lenght faster by epoch 50\n",
    "1000 step episodes at epoch 140 and derailed\n",
    "\n",
    "For all above there was an issue with compute advantages where the first advantage of an episode would be set to zero. Probably had little impact.\n",
    "\n",
    "3/31\n",
    "Multiple tests with -0.001 living reward failed to learn. Vf loss went to 0, but episode length stayed at 1000 and reward at -1.25.\n",
    "Removing living reward appears to work better at first and then back to failing. Not sure what changed since last living reward run.\n",
    "\n",
    "Increase warmup to 40 epochs. At 50 epochs, reward was back to -0.25\n",
    "\n",
    "Hyperparams from openai train-procgen\n",
    "lr 5e-4, ent 0.01, gamma 0.999\n",
    "Did better at first fell back to zero\n",
    "\n",
    "1e-4 never gets above 0.\n",
    "\n",
    "3 ppo epochs per game loop. Worked until epoch 242 and then all zeros.\n",
    "-1e-4 living reward did not help, -0.25 term reward did not help.\n",
    "\n",
    "maxStaleSteps 64, same\n",
    "1 agent, same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ac488eaa353570522d4c04bd2cd8e3c67c3437ec54aafb89a01cbc7941828458"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
